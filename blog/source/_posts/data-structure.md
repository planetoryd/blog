---
title: Some data structures of dweb
date: 2021-02-20 14:32:04
tags:
    - draft
    - dweb
---


# Structuring and linking the web

Traditional internet are often scattered although weakly connected with hyperlinks, which leads to the presence of web crawlers. The sites intend to fight with crawlers, for some reason, but most of the sites shouldn't have the need to protect their content. The reality turns out that crawlers are still wasting energy to get content from some site. They should have exposed part of their database to the public to avoid the non-sense of developing spiders, which is not possible of course. In The Network, we don't even have database, which is called **No-database**. All the data are organized and stored locally, so there won't be any need of spiders, and the protection of content will be infeasible. This is first barrier removed. No-database means we have our own data structure based on IPLD, which doesn't turn to a conventional database, but uses the fittest solution for dweb and its new requirements. Why using the spiders as an example ? People want information, and it tend to be exchanged, so the structure without the struggle of getting information is most efficient.

Hyperlinks are the only links present, as mentioned. Intuitively, the lack of links reduces the value of a network as the cost of reaching some content is too large, and raises the need of a search engine, further centralizing the web. Then, how do we evolve the links ? Links are the related items for some specific item, because we don't link unrelated things, and related things are more likely to be useful for the users. We should also classify the links, into some groups, such as origins, dependencies and resources, because classification is equal to creating a new subnode, and when it is known what we kind of links are looking for, the probability is greater this way. This is called **semantic-linking**, in The Network. Links **respect the original author**. Opposing copyright doesn't mean to disrespect the authorship. Instead, the linking makes it even easier to track some piece of work, listing all of the authors involved. The natural selection of content encourages users to re-use content and use links, but more importantly, the applications have the feature of such linking that didn't exist in the past. The links relate content, as a kind of additional information, for de-duplication, reference, and even user-specified links. Then we will have meme pictures along with the source urls, and a network of references presented when we read an article or watch video. Now there're four networks, the DHT that links users mathematically, the DHT that links the content, the trust network of users, and the links of content.

In contrary, corporations do neither of them, and even create barriers and smash the tiny clues showing who the author is. They don't have the no-database structure simply because they are not P2P and so the centralized web naturally duplicates, which they have nothing to doï¼Œ but sometimes they do that on purpose. That phenomenon is called 'content laundering', which is a typical result of capitalistic advertising economy. People create quality works and just sink into the bottom of internet until their works are stolen. On the other side, readers usually get what they want to read, no matter whether the content are laundered or not. This actually has difference from directly seeing author's content, that is people have to bear the advertisements and the low-quality of 'laundered text', and lose the opportunity to talk about the article with the author. Let's think about a situation where an issue has to be solved, but the discussion takes place on two separate platforms and people don't about each other, which causes either group cannot see each other's opinions when they could both have reached the optimal solution if the discussion happened on one platform.

# Natural selection of content

This is already happening on almost all kinds of p2p networks (the exception is Freenet). The more needed some content is, the more replica it has, which is totally natural. In centralized web, the content has to have the potential to generate money, so it is not the most needed content that survive, but the one generating most money which is not necessarily what people want. For example, the phishing sites which obviously nobody wants, but they do generate money for the hosting cost. However, is the natural selection of content really good ? Now if you take look at **torrenting**, you will find that **most content on it is videos**, which are mainly movies. The consequence of our network would be like this if we didn't control it. Movies are indeed most people want, but they would choose other kinds of media if there were. At the very first time people use torrents, the inconvenience of reading books from it caused people to use it for movies, and the natural selection of content makes it harder and harder find some books as books are seldom seeded and so no one would look for books on it.

BitTorrent protocol also has some algorithm to make less-seeded content more prioritized when seeding, which is an example of countering this natural selection, although such mechanism actually brings truly some unwanted content back. The natural selection is good, but we should do something for the future of the network. To encourage reading and writing text, which is very small in data size, we can make every client automatically download such content in WoT. We should extend such strategy to a larger scale. The users can't start downloading something if others don't do the same thing at the same time, for there isn't enough seeders. The downloading should start from the nearest peer of some content (in WoT or mathematically). So, we should set up something like 'e-book seeding campaign' which a user can choose to join, and e-book subscription channel which seeds new ebooks the user probably prefers. Sometimes we should even make decisions for users whenever the circumstance allows, which benefit the entire network in short-term or long-term. Although that is a decentralized network, it doesn't mean that we lost control of it. We should **plan the network to seed** some file before it is exposed and demanded. This is can be seen as 'predicting demand' or 'inspiring demand'.

> Seeding means to serve some content so that other users can get it.

The natural selection of content isn't necessarily better than a platform that keeps all the content, but it is the unique way of dweb. In most cases, the natural selection doesn't happen, unless some content is poor enough that most users choose to remove it. What is shown to users is determined by the site which can have the algorithm of machine in a decentralized form. The natural selection of content is about the allocation of seeding resources, the storage space, replication, and time, which will take a leading role only when there is no enough resources for seeding. However, we always have limited resources while infinite content can be produced, so the choices of what content should remain will always be made.