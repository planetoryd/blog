{"meta":{"title":"planetoryd","subtitle":"the blog","description":"a blog about the decentralized web","author":"Planetoryd","url":"https://planetoryd.github.io","root":"/"},"pages":[{"title":"Repositories","date":"2021-02-20T06:24:02.624Z","updated":"2021-02-20T06:03:05.000Z","comments":false,"path":"index.html","permalink":"https://planetoryd.github.io/index.html","excerpt":"","text":""},{"title":"404 Not Found：该页无法显示","date":"2021-02-20T06:26:27.705Z","updated":"2021-02-20T06:03:05.000Z","comments":false,"path":"/404.html","permalink":"https://planetoryd.github.io/404.html","excerpt":"","text":""},{"title":"关于","date":"2021-02-20T06:26:27.705Z","updated":"2021-02-20T06:03:05.000Z","comments":false,"path":"about/index.html","permalink":"https://planetoryd.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"分类","date":"2021-02-20T06:25:32.719Z","updated":"2021-02-20T06:03:05.000Z","comments":false,"path":"categories/index.html","permalink":"https://planetoryd.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-02-20T06:26:27.693Z","updated":"2021-02-20T06:03:05.000Z","comments":false,"path":"tags/index.html","permalink":"https://planetoryd.github.io/tags/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2021-02-20T07:17:37.616Z","updated":"2021-02-20T06:03:05.000Z","comments":false,"path":"repository/index.html","permalink":"https://planetoryd.github.io/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"流量经济之求生指南","slug":"流量经济之求生指南","date":"2021-03-18T14:48:03.000Z","updated":"2021-03-28T06:39:45.456Z","comments":true,"path":"2021/03/18/流量经济之求生指南/","link":"","permalink":"https://planetoryd.github.io/2021/03/18/%E6%B5%81%E9%87%8F%E7%BB%8F%E6%B5%8E%E4%B9%8B%E6%B1%82%E7%94%9F%E6%8C%87%E5%8D%97/","excerpt":"","text":"想必现在的状态离理想的互联网图景还很遥远，在当下我们该做什么？ 求生必然意味着我们没有选择，所有内容都来着这个生态体系，我们总不能作茧自缚，不摄取外界的信息吧。 在流量经济体系外的，公益性质网站如Wikipedia，不够发达但仍可使用的Torrent，都是首选的信息来源。 流量经济内部，也分几层，最劣质的是纯粹由广告驱动的系统，各类软文，不如一概不看。 国内与国外也有区分，国外的流量经济发展程度优于国内的，因此少看百度的内容为宜。 既然互联网已经被垄断，不如就此接受，选择垄断企业时也应当挑最大的，比如看Youtube而不看国内的视频网站。虽然大部分内容琐碎，但主动查找还是能发现高质量内容的。 我们应该接受广告吗？不必，接受广告是一种妥协。即便我们不开发分布式互联网，当人的思想转变，人也能主动避免陷入流量经济的泥潭。 拒绝广告，在一个方向推动内容与广告的融合，另一个方向也在警醒充满贪念的内容作者，因为创作本身应当是以丰富人类信息为目的，而不应当是经济利益。 我们大可不必担忧经济来源，因为这样的做法减少流量经济网站的访问量，流量转向非盈利网站，就推动这类网站的发展。同理，软文的流量减小，那些无私作者写的文章流量就增加。 寻找软件时关注Github，F-Droid，而不是各种应用商店。 如果说要一个例子证明思想能扭转流量经济的，那这个例子一定是Github。显然Microsoft是不会做公益的，是人民的思想迫使它的。 我们确实应当重视思想的，分布式网络只是一个捷径罢了。 尽量接触内容源，避免接触多次传递的信息，信息转手的次数越多越失真。","categories":[],"tags":[{"name":"advertising","slug":"advertising","permalink":"https://planetoryd.github.io/tags/advertising/"},{"name":"final","slug":"final","permalink":"https://planetoryd.github.io/tags/final/"}]},{"title":"流量经济及其解决方案：中文简介","slug":"流量经济及其解决方案：中文简介","date":"2021-03-16T09:52:52.000Z","updated":"2021-03-19T11:20:39.165Z","comments":true,"path":"2021/03/16/流量经济及其解决方案：中文简介/","link":"","permalink":"https://planetoryd.github.io/2021/03/16/%E6%B5%81%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%EF%BC%9A%E4%B8%AD%E6%96%87%E7%AE%80%E4%BB%8B/","excerpt":"","text":"整理了之前的部分论点，但可能还会有遗漏。 你可能不会关注到这个问题，从百度或者谷歌搜索各种东西、从应用商店下载一些应用、去书店或者网上买书，你把这些事情当做常态。这个题目甚至是高考的题目，你可能听说过，然而实际他们是没有解决方案的。你理所当然地认为广告是网站生存的必需品，纸质书、各种电子书平台是必要的回本方式。这实质上是必然的，因为流量经济几乎渗透到了互联网的所有角落，以至于用户则甚至根本不会意识到它的存在。你可能会认为我小题大做、危言耸听，但流量经济绝不是让你看个广告、恶心一下你就结束了的。一旦广告成为互联网的主要营收来源，整个互联网的行为也随之改变。 流量经济的运作机制很简单，广告主付钱让各个网站打广告，常见的计费方式即，用户看到广告、点击广告就计费。计费方式如何并不重要，只要广告有效果，就会有人愿意打广告。对于广告这种行为本身，最表面的效果是影响用户体验，这也是各个平台一直在解决的问题。广告更大的问题是效率低下，一千个人中有十个看了广告，十个中又只有一个人购买了产品，这对于广告主是经济的，对于用户，是对时间和注意力彻底的浪费，因为广告主所有的成本都是用户共同承担，而所有商家都打广告，因此用户没有选择。换句话说，用户自己买罪受，看一篇文章要受一大堆广告折磨，而且买的商品里还加上这部分广告的成本。流量经济的发展，可谓是不断改进用户体验的过程，然而改进的仅仅是用户体验，流量经济反而越来越差了。在几年前互联网还没现在这么发达，网站打广告的方式很简单粗暴（例如各种下载站），随便什么文章，中间插满广告，悬浮、弹窗广告，利润很丰厚，表面上用户体验很差，但对人的效益实际上比现在这种状态好。我们势必要理清广告是什么，广告的本质是夺取用户的注意力，你看到某个广告，说不定就买了，另一个是品牌的曝光。在有多个竞争对手时，需要的就是优势了，对于某个品牌更多的曝光，于是现在的广告形式变成“竞价排名”，以百度为代表。从简单直率的广告，到隐晦的广告，本质的变化是内容与广告逐渐合为一体，代表产物还有“软文”。当广告与内容合为一体，真理就会被扭曲，这个相信每个人都能切身体会到，找资料是有多难了。你想查一个问题，结果搜出来都是软文，互相抄袭，毫无可信度。你想下载一个软件，最先看到的都是各种钓鱼网站。 流量经济有病毒一般的传染力，当一个品牌选择打广告时，竞争对手不可能坐视不理。不管当时谁想出这么个往网页上堆广告的想法，流量经济就此开始扩张了。从互联网发展来看，流量经济确实有为内容作者提供资金的作用，毕竟没几个人有捐款的思想觉悟。流量经济发展到现在，已经产生了不同于初期的特征。这里必须再提一下互联网的自然垄断效应，用户越多，平台的价值不是线性增长，而是指数级增长，所以小的平台在同等条件下基本被挤死。流量经济和自然垄断效应共同作用，造就了今天的局面，几个互联网寡头企业垄断内容经济。同时，流量经济病毒般的胁迫力又迫使各行各业给它输血。 Ad-block可谓是用户一次天真而不成功的反抗，这个现象的产生直接推动广告经济从初期的直率演变为当今的状态。之所以天真，原因很显然，屏蔽广告不能解决内容作者的资金来源问题。屏蔽了表面的广告，其资金来源必然依靠更加隐晦的广告手段。无论用户如何反抗，流量经济能提供的只有另一种接受广告的方式。以前是大量的无效广告信息中附加明确、有效的信息，现在是广告信息和有效信息交杂在一起，全世界的用户显然都选择了后者。这样交杂的方式亦是Ad-block不能解决的类型。这也意味着，改变流量经济之现状需要另外一种体系，另外一种内容生态系统同其竞争。目前我已知的方式大约只有分布式网络了。 内容是任何“经济”的核心。现在的各种“经济”，都只是把内容当做商品，即先付钱再阅读。如果是先阅读再付钱，甚至用户可以选择是否付钱，这就变成捐赠了。从读者的角度看，我们失去了获取信息的自由，或者说信息不能自由的传播。你不能说书店里让你看两下就是先阅读再付钱了，因为这样的阅读是受限的，付钱之前的阅读自由一定是不如付钱后的阅读来得自由。或者说，读者为书籍付钱，就是为了达到付钱后阅读的自由度。当我们确定大多数读者不会自发捐款，而读者为书籍付钱，可以推断出内容的自由一定是受限的。极端的例子就很明显，成功学书籍、教辅书籍，乍一眼看上去很好，内容编排精美，付了钱看了两个月后才发现都是废话。这其实就是内容不自由的结果，内容的传播被金钱左右。垃圾书的作者在你付钱的时候得到了“即时”的奖励，你两个月后的评价和他毫无关系。相比之下，分布式网络的特点是需要做种，你的评价是持续的、有效的。 从开源，到知识共享，这个自由的精神其实已经发展了很久。这时我提出涵盖面更广的概念，即内容自由（涵盖二者且概念更广）。内容本身不能自由，这里指的是内容生产的自由、传播之自由、消费之自由，也就是整个内容生态的自由。给予内容自由的是人，就好比你如果不分享自己的资源，别人不可能拿到一样。实现内容自由，必然要靠人主动的行为，分布式网络提供的就是这样一个分享内容的机会。其中最特别的概念是传播之自由，是开源、知识共享都没有涉及的。内容的自由传播是让优质内容传播的更广，而流量经济的效应是使经济效益高的内容传播的更广，这自然违背了内容传播之自由。二者产生区别，是因为高质量内容不见得能带来更高的经济效益。被迫在内容上附加广告，可以理解为剥夺内容生产之自由，因为写无广告、不凑字数的内容不能带来收入。分布式网络的内容传播完全由用户决定，内容在用户之间直接分享，于是用户有能力选择分享何种内容，拒绝哪些内容。 什么内容金钱效益高呢？降低写作的成本，提高点击率，多扭曲一些内容增加广告，这些我们统称低质内容，诸如一些猎奇的文章，夸张的标题，乱七八糟的新闻。让资本家高兴的是，有些内容甚至看了让人上瘾，于是很多人的时间就有意无意地被浪费掉了。我们不能说是用户对低质内容上瘾，去批评学生不专心学习，倒不如说这个互联网内容太匮乏了，因为内容生产者总量不变，流量经济的发展必然挤压其他内容的空间。而高质量内容本身就少，就淹没在流量经济之中了。 流量经济是当今互联网无序的体现，解决这个问题需要用另一种方式将人组织起来，即分布式互联网。 至此其实还没讲完，解决方案几乎没有提到多少。下一篇可能是，分布式网络社会。刻意省略了很多细节，避免写得太冗长。考虑另起文章讨论。改日甚至可以写一篇文章叫什么，The Decentralist Manifesto，然而现在还不敢取这样的标题。","categories":[],"tags":[{"name":"advertising","slug":"advertising","permalink":"https://planetoryd.github.io/tags/advertising/"},{"name":"dweb","slug":"dweb","permalink":"https://planetoryd.github.io/tags/dweb/"},{"name":"final","slug":"final","permalink":"https://planetoryd.github.io/tags/final/"}]},{"title":"The advertising of the current web","slug":"advertising","date":"2021-02-20T06:32:04.000Z","updated":"2021-03-18T12:36:45.225Z","comments":true,"path":"2021/02/20/advertising/","link":"","permalink":"https://planetoryd.github.io/2021/02/20/advertising/","excerpt":"","text":"Advertising Capitalistic advertising: the means of communication are owned by capitalists, who then started such advertising economy, which is utterly different from the positive way of advertising that will happen in the dweb. The capitalistic advertising composes almost the entire internet and sucks everyone’s attention and time, through the monopoly of search engines. Capitalistic advertising encourages false rumorsClickbait is the unique phenomenon of capitalistic advertising. Because the writers of clickbaits will be instantly rewarded after the users have clicked their articles, they don’t care what they have written or whether the content is true or not. The internet is anonymous and lacks trust, so they won’t be regulated or punished even if they have done something immoral. Moreover, internet can’t be completely regulated, for the ineffectiveness of governments and the complexity of it. It is not realistic to expect the regulators to make the whole capitalistic advertising system better. The rumors are mostly the results of it, and the starters of these rumors are rarely found because of te anonymity. It disrepects innovationMost content we see are the result of the capitalistic advertising, since the content that are not funded by the system are automatically ranked lower than the funded one, and the writers of the true quality content lack income. Even though some people have written anything worthy, their works will soon be stolen and mixed with advertisements through a process of content laundering. The origins of the content will be smashed, for the content thieves are afraid of being spotted, and they simply don’t care about the valuable contribution of the original authors. As mentioned before, the writers are encouraged to writer because of the rewards of advertising. They do little innovation in their content, but only steal others’. Then everyone tends to steal the works written by others, who are to produce the works that will be stolen ? The quality-content writers’ content are not being shown to readers, but the writers that advertise get attention. Therefore, the centralized web is fulled of repetitions and some seemingly innovative content that are only innovated in terms of being more attractive to readers but nothing worthy inside the words. It produces and promotes titytainment Titytainment is a kind of content that gives people satisfacation in a short time. Titytainment, as a phenomenon, is arguably common, which has drained the time and efforts of many people. The point is not to get rid of this, as entertainment is not wrong, but we do not have the way to get non-titytainment content as easily as how we get the former. For example, I can’t find an article talking about what I am talking right now here with google even though the field of dweb has been developed for 20 years, which is considered quality content by me. When we have the ability to get such in-depth content, I believe, not everyone would ask for titytainment. What about going to libraries or reading some e-books ? Apparently it is a valid point that I’m lazy so I chose titytainment, which is partially right for people are indeed lazy, but the reason for not reading in-depth content is not lazy, as most of them are willing to read better content when they are as available and reachable as titytainment. Going to libraries or reading e-books is just too old-fashioned, especially when we are talking about dweb. The long publication cycle, cost and lack of interactions, and the outdated form of e-books, are not even written about here. Even for titytainment, dweb does it better, because whenever a meme or something goes viral, the seeders skyrocket and no one stops it from getting attention, and we can absorb content from other platforms. Such content remain as long as the majority need it, who do in this assumption. The key demand of titytainment is diversity and time, which fits anyone’s interest and is updated in time to meet the desire of seeing new content. Then we are going to change the users’ habit by feeding them better content, which a media would never do. The result of the dominance of means of communication by capitalistsWhat do I mean by means of communication ? I was trying to find a book that truly has some worthy content, and it turned out that both online platforms and book shops have only those filled with non-sense. I bought the one recommended by a friend of mine, which is obviously better than what I got from online recommendations. Now you have more friends with different occupations and specialties, and friends of friends, via The Network. Then our choices will be different, which is one of the effects of the transformation of the means of communication. Another effect is the elimination of capitalistic advertising, mentioned elsewhere. The effects of changing the means of communication also include getting true freedom of speech, which we don’t really have right now. It is suppressed by both western capitalists and eastern elites, which some people believe is the right way. At some extent, they are right, for average people are usually misled by media, and countries fight each other by misleading each other’s netizens. All of them never sought to educate the netizens, which is understandable, however, only when there’s no dweb. The big media backed by big countries, and small ‘self-media’ supported by advertising, are all biased from the perspective of readers, on condition that all of the parties don’t encourage you to get information from any other party, the result of which is the failure of communication of global netizens. In other words, the evolution of internet is the evolution of the means of communication, but mostly it evolved in terms of convenience, as you see how internet goes faster, smarter and more widely applied, the fundamental part of which hasn’t been changed for decades. (refer to the evolution of trust in another article). What advertising should be likeMost advertisements don’t really work, unless some company has advertised for a long time repetitively which forces it into customers memory. The former is nonsensical as we seldom remember it when we need some product, but the latter is worse which is inhumane. What kind of product needs such advertising ? Most probably it is a kind of platform which wants to dominate the market with infinite advertisements in a short time. Except for this kind of products, the usual ads should be effective on some people, some of whom tolerate ads and others don’t. We have nothing to do with the people who tolerate ads. Anyway the majority don’t, so the anti-adblockers and anti-adblocker-killers are at constant war. The industries supported by ads still work, with their clever tricks like forcing users to install apps where you can’t use ad-blockers(who’d expect users to root the phone and get edxposed). It doesn’t actually work, although it seems. It is a failure of communication with consumers. It remains because no one, no company or government can change it, for the anarchism of internet. The advertising industry can be only killed by technology itself. People tend to find products when they need them, but not when they are not interested and not willing to recite the names of some product, and the ads have cost as well which will be paid by consumers in the end. Therefore, the ads should only be shown when the users want which should also be selected by the WoT. What if a user doesn’t know its need ? Well, this does exist, but the first need is not to see an ad. An ad considered an ad when reading an article is not necessarily an ad when shopping on dweb. To be classified as an ad on a shopping site, it has to be biased, such as paid advertising. The definition of ad changes when the circumstances differ.","categories":[],"tags":[{"name":"draft","slug":"draft","permalink":"https://planetoryd.github.io/tags/draft/"},{"name":"advertising","slug":"advertising","permalink":"https://planetoryd.github.io/tags/advertising/"}]},{"title":"Strategies of dealing with censorship","slug":"anti-censorship","date":"2021-02-20T06:32:04.000Z","updated":"2021-03-18T12:36:35.423Z","comments":true,"path":"2021/02/20/anti-censorship/","link":"","permalink":"https://planetoryd.github.io/2021/02/20/anti-censorship/","excerpt":"","text":"The purpose of opposing censorshipFor me, the purpose is to give an opportunity for our project, or many of the users will not be able to use it. In western values, it is about their free speech, and in socialist values it is encouraged but not seriously implemented. The purpose here is about neither of them. Opposing censorship is merely to create the possibility of practising an ideal freedom of speech, the word ‘possibility’ among which is to say we shall at least have the ability to get free speech which is the prerequisite of all other things. We should create it first before thinking or criticizing free speech. Anyway, you can’t criticize such dweb free speech before it actually appears. From another perspective, decentralized networks are just too hard to ban after they have been created, so it makes no actual sense to oppose it. Dweb free speech contains the ability to neglect existing laws, such as intellectual property laws, and national security laws. The old means of communication have to be conquered before the revolution happens. Socialist countries often have stricter suppression of free speech, in which we have a larger undeveloped market where those foreign capitalists can’t access, and people there will be more motivated to use The Network since domestic monoplies are often unsatisfying. How toMesh networks are often mentioned as an ultimate way to counter censorship, which is in fact impractical especially in socailist countries where the physical control of cities are relatively higer, but this should be the networking of the future. The general idea of avoiding censorship is to pretend to be normal traffic, making it indistinguishable from legal traffic which matters to economy. Most censorship-bypassing services in China are set up in this way. Others use private line, avoiding to pass through GFW. There’s always a way to disguise as normal traffic, and no one knows who are using and what is transferred in some seemingly normal traffic. Assuming China has blocked all peer connections and only verified servers can be dialed, which is an extreme situation that harms internet infrastructure and economy, what we can do is to run a blockchain that pays people who have the access to those servers to transform them into our relay servers and who are willing to punch holes on this system. However, China is not Russia, which cares about economy. It is unlikely that this assumption comes true. The circumstance for dweb will only be better, as internet develops. China can block peer connections, but they can’t block international connections. International connections are the other way out even if the domestic connections are blocked, which can be between any nodes in China and any nodes with public IP outside. It becomes ‘any nodes outside’ simply because the government won’t and can’t verify every foreign website which includes some websites of trade partners. But why almost none of torrenting softwares survive in China ? The problem is not about protocol, but about human power. These torrenting softwares failed to fund themselves, which is the issue we aim to solve and has been addressed elsewhere. The technology of censorship is constantly evolving, while we can always find a way out with patches in time. The task in western countries is much easier, but with emphasis on anonymity due to their ability and experience on striking copyright infringement. Then, the western users can help users in countries having censorship. As the fundamental requirement of dweb, censorship-resistance and connectivity is the top concern. According to the team members of IPFS, one kind of dweb, they seek to get commercial adoptions and wide applications of Filecoin, which is the way of manually creating the burden of censoring for the government. The future reaction of government is unknown, but our dweb share the same protocol with IPFS, it will be benefited too.","categories":[],"tags":[{"name":"draft","slug":"draft","permalink":"https://planetoryd.github.io/tags/draft/"},{"name":"censorship","slug":"censorship","permalink":"https://planetoryd.github.io/tags/censorship/"}]},{"title":"Some data structures of dweb","slug":"data-structure","date":"2021-02-20T06:32:04.000Z","updated":"2021-03-18T12:36:03.027Z","comments":true,"path":"2021/02/20/data-structure/","link":"","permalink":"https://planetoryd.github.io/2021/02/20/data-structure/","excerpt":"","text":"Structuring and linking the webTraditional internet are often scattered although weakly connected with hyperlinks, which leads to the presence of web crawlers. The sites intend to fight with crawlers, for some reason, but most of the sites shouldn’t have the need to protect their content. The reality turns out that crawlers are still wasting energy to get content from some site. They should have exposed part of their database to the public to avoid the non-sense of developing spiders, which is not possible of course. In The Network, we don’t even have database, which is called No-database. All the data are organized and stored locally, so there won’t be any need of spiders, and the protection of content will be infeasible. This is first barrier removed. No-database means we have our own data structure based on IPLD, which doesn’t turn to a conventional database, but uses the fittest solution for dweb and its new requirements. Why using the spiders as an example ? People want information, and it tend to be exchanged, so the structure without the struggle of getting information is most efficient. Hyperlinks are the only links present, as mentioned. Intuitively, the lack of links reduces the value of a network as the cost of reaching some content is too large, and raises the need of a search engine, further centralizing the web. Then, how do we evolve the links ? Links are the related items for some specific item, because we don’t link unrelated things, and related things are more likely to be useful for the users. We should also classify the links, into some groups, such as origins, dependencies and resources, because classification is equal to creating a new subnode, and when it is known what we kind of links are looking for, the probability is greater this way. This is called semantic-linking, in The Network. Links respect the original author. Opposing copyright doesn’t mean to disrespect the authorship. Instead, the linking makes it even easier to track some piece of work, listing all of the authors involved. The natural selection of content encourages users to re-use content and use links, but more importantly, the applications have the feature of such linking that didn’t exist in the past. The links relate content, as a kind of additional information, for de-duplication, reference, and even user-specified links. Then we will have meme pictures along with the source urls, and a network of references presented when we read an article or watch video. Now there’re four networks, the DHT that links users mathematically, the DHT that links the content, the trust network of users, and the links of content. In contrary, corporations do neither of them, and even create barriers and smash the tiny clues showing who the author is. They don’t have the no-database structure simply because they are not P2P and so the centralized web naturally duplicates, which they have nothing to do， but sometimes they do that on purpose. That phenomenon is called ‘content laundering’, which is a typical result of capitalistic advertising economy. People create quality works and just sink into the bottom of internet until their works are stolen. On the other side, readers usually get what they want to read, no matter whether the content are laundered or not. This actually has difference from directly seeing author’s content, that is people have to bear the advertisements and the low-quality of ‘laundered text’, and lose the opportunity to talk about the article with the author. Let’s think about a situation where an issue has to be solved, but the discussion takes place on two separate platforms and people don’t about each other, which causes either group cannot see each other’s opinions when they could both have reached the optimal solution if the discussion happened on one platform. Natural selection of contentThis is already happening on almost all kinds of p2p networks (the exception is Freenet). The more needed some content is, the more replica it has, which is totally natural. In centralized web, the content has to have the potential to generate money, so it is not the most needed content that survive, but the one generating most money which is not necessarily what people want. For example, the phishing sites which obviously nobody wants, but they do generate money for the hosting cost. However, is the natural selection of content really good ? Now if you take look at torrenting, you will find that most content on it is videos, which are mainly movies. The consequence of our network would be like this if we didn’t control it. Movies are indeed most people want, but they would choose other kinds of media if there were. At the very first time people use torrents, the inconvenience of reading books from it caused people to use it for movies, and the natural selection of content makes it harder and harder find some books as books are seldom seeded and so no one would look for books on it. BitTorrent protocol also has some algorithm to make less-seeded content more prioritized when seeding, which is an example of countering this natural selection, although such mechanism actually brings truly some unwanted content back. The natural selection is good, but we should do something for the future of the network. To encourage reading and writing text, which is very small in data size, we can make every client automatically download such content in WoT. We should extend such strategy to a larger scale. The users can’t start downloading something if others don’t do the same thing at the same time, for there isn’t enough seeders. The downloading should start from the nearest peer of some content (in WoT or mathematically). So, we should set up something like ‘e-book seeding campaign’ which a user can choose to join, and e-book subscription channel which seeds new ebooks the user probably prefers. Sometimes we should even make decisions for users whenever the circumstance allows, which benefit the entire network in short-term or long-term. Although that is a decentralized network, it doesn’t mean that we lost control of it. We should plan the network to seed some file before it is exposed and demanded. This is can be seen as ‘predicting demand’ or ‘inspiring demand’. Seeding means to serve some content so that other users can get it. The natural selection of content isn’t necessarily better than a platform that keeps all the content, but it is the unique way of dweb. In most cases, the natural selection doesn’t happen, unless some content is poor enough that most users choose to remove it. What is shown to users is determined by the site which can have the algorithm of machine in a decentralized form. The natural selection of content is about the allocation of seeding resources, the storage space, replication, and time, which will take a leading role only when there is no enough resources for seeding. However, we always have limited resources while infinite content can be produced, so the choices of what content should remain will always be made.","categories":[],"tags":[{"name":"draft","slug":"draft","permalink":"https://planetoryd.github.io/tags/draft/"},{"name":"dweb","slug":"dweb","permalink":"https://planetoryd.github.io/tags/dweb/"}]},{"title":"Rethinking the protocol","slug":"design","date":"2021-02-20T06:32:04.000Z","updated":"2021-03-18T12:35:57.538Z","comments":true,"path":"2021/02/20/design/","link":"","permalink":"https://planetoryd.github.io/2021/02/20/design/","excerpt":"","text":"Design of the protocols 2021 The diagram shows the general evolution of related projects. Arch Protocols present Bitswap with DHT Only works if the blocks are static, immutable, since the records on DHT are not to be changed Gossip Not suitable for large amounts of data (An object can’t exist without a site) Site Object EncodingTo some specific peer Utility rate of the base block by the current block Overall utility rate of the base block Possiblity of being required again by other blocks Possibilty of requring the blocks that require the base block Difficulty of fetching Size Seeders To designThe common issues of internet, and its dweb solution Data reuse, deduplication IPLD Archival Immutability of IPLD Object archival Convenience, unification, security Site runtime Hyperlink, and beyond it Semantic links Code reuse Inter-site reuse, component, template, etc. IPLD (as the way of reusing the code itself) Simplicity (of the network itself and the development of applications) IPLD as an alternative to filesystem Less repeating concepts ArchiveThe space and bandwidth of each peer is limited ⇒Archive to save space and bandwidth for the new versions The bandwidth is saved and the speed of fetching new versions is raised only when the majority have stopped seeding the older versions, and the new version is preferred by the majority ⇒Archival must be done after negotiation at the same time Archival ⇏ Saving space or bandwidth solid compression","categories":[],"tags":[{"name":"draft","slug":"draft","permalink":"https://planetoryd.github.io/tags/draft/"},{"name":"dweb","slug":"dweb","permalink":"https://planetoryd.github.io/tags/dweb/"},{"name":"diagram","slug":"diagram","permalink":"https://planetoryd.github.io/tags/diagram/"}]},{"title":"Next steps of dweb-revolution","slug":"dweb-business","date":"2021-02-20T06:32:04.000Z","updated":"2021-03-18T12:35:50.808Z","comments":true,"path":"2021/02/20/dweb-business/","link":"","permalink":"https://planetoryd.github.io/2021/02/20/dweb-business/","excerpt":"","text":"A top-down reform by capitalists only kills dweb, since average users will have no motivation of turning to it. This is actually happening, Microsoft are trying to accept open-source, Google, Youtube are constantly improving their promotion algorithm, and even the corporations behind WeChat in China have started to realize the problem and change their strategy of business. Therefore, a revolution before them is necessary. What is the revolution about ? Exactly, the piracy with some more things. The first step is to conquer those existent intellectual property we have on the web, which naturally attracts users for the next step. There are various reasons why former piracy practices didn’t do well, that all of the content were solely pirated with no original content being published, which stops users from using dweb on a daily basis. The content were only pirated movies, music, and books, so the system would eventually break for it gives nothing back to the authors. Then, we would establish a dweb economy which raises funds via blockchain for movies, and offers a platform of donating. At last, it will succeed with all these new ideas combined. How ? The key of the revolution is to get users, which is also a part of step one. We can set up a market to pay people to grab content from other places, whether legal or illegal, and the platform forces them to share the fruits with all peers. This way any copyright protection system will just break, as no one, or not all of the people would refuse the rewards, when it is nearly impossible to find out who has leaked the copyrighted stuff. The first ones who want something will pay for the fee, and then it will benefit all. The capacity of network will expand as the users grow as well as their needs, so there won’t be the problem of storage. Afterwards, the monopoly of the centralized web will have been broken, and everything will be ready for us. When the network has the ability to gather enough people to raise funds, we will establish some official maintenance groups, governed via our blockchain or site system, to continually fight against governmental censorship by developing new obfuscation protocols. In the same way, we can run the same business we currently see and use. When it comes to payment, we can set up a exchange platform between fiat money and crypto-currencies, which will work more securely with WoT. Promoting the Dweb for more adoptionWe all know the benefits of dweb to the progression of society, but the prerequisite is the mass adoption of dweb, which has not been achieved for a long time. As a kind of software that heavily relies on users, previous projects have failed for various reasons: insufficient maintenance which causes the software to be unusable, zero-userfriendliness with no documentation, and the final abandonment of the project after the exhaustion of passion. The most well-known network is Torrent, which thrives on the advancement of DHT, reaching quarters of a billion users monthly. Arguably, most use of torrent is for movies, which is the fastest type of files to download. The user-base grows as the need of movies and other videos grows. Movies are only a part of the internet, although the two do occupy much portion. The sole usage of torrent is getting content with no personal interaction among users, which actually lessens the total value of torrent though having such number of users. From my past experience, BitTorrent profits from the ads in their clients, and they probably collaborate with VPN providers. Neither of the ways is tolerable to us. The natural way is to let the users promote, which has been effectively blocked to Bittorrent since they have put on advertisements. After the promotion of our software, we shall cultivate the habit of using dweb. Many users stop using dweb after a few attempts, when they have found it difficult to use or boring. The former could be caused in various ways. In censored countries, the difficulty to break the barrier of connectivity is unbearable for an average user, which we couldn’t do much. Another is user-friendliness, for which we will set up a decentralized reporting platform. The network can’t be boring if it has covered all parts of today internet, from video streaming to instant messaging. The strategies of trapping users are best applied by those Chinese companies, which include making a non-sense browser on the basis of chromium with tons of advertisements, and shipping products with unrelated messaging platforms, social networks or even a complete news reader. For example, WeChat has a payment system, Alipay has instant messaging feature, and there’s a ‘discovery’ section in Meituan that has all aspects of videos from news to entertainment. The annoying part is that all of them fill up your notification panel whenever possible. Although this is immoral, it works, which is the reflection of users who tolerate and even accept it. The only fault is that we have no choice under the monopoly, as they only care about the majority in the market who generates most profit. Therefore, it is vitally important to make a browser including both desktop and mobile versions and run the dweb like how centralized web gets users. PlatformsFinally, we will migrate most types of applications to dweb. Platforms in which dweb is advantageous are firstly developed. The purely online platforms are the easiest ones which are not mentioned here. It becomes more challenging to deal with actual things, for platforms such as a decentralized online shopping site, which will go beyond darknet markets. The dweb shopping sites not only will be benefited from the content selection effect of dweb, but also has the WoT, which is like that you buy things from someone in the distance recommended by your friends, and this is different from physically recommending goods which is ineffective and no one would actually do this while machine does such boring work. The comments towards some goods will never be the same ones you see in centralized web which are mostly posted by bots or paid commentators. Next, we can make further complicated platforms, such as renting houses or cars, dealing second-hand goods, which are more time-consuming when and before dealing, to eliminate the existence of intermediaries. How do we run such business ? We don’t have running costs, all users can be moderators, and the algorithm of dweb works. These are all platforms which are mainly run by moderators who are chosen from the users. Besides platforms, we have blogs and small sites by individuals and utility sites. Then some small companies who think our dweb has a decent amount of users and don’t want to pay for the ads will set up a site here. At that moment dweb will start to grow. How The Network will remain libre Libre is beyond free and opensource. Many great opensource projects were discontinued or became ‘evil’ at last. It wasn’t noticeable but it did happen that Firefox China made a Chinese version firefox that has pre-installed annoying add-ons and homepage, which is an enhanced form of annoy compared with the global one since Chinese users usually have more tolerance. Today, the majority of Mozilla Corporation revenue is generated from global browser search partnerships, including the deal negotiated with Google in 2017 following Mozilla’s termination of its search agreement with Yahoo/Oath (which was the subject of litigation the parties resolved in 2019.) The composition of revenue directly indicates that Mozilla isn’t libre, although being opensource and free. It has to maintain the partnership with Google who then run the advertising economy and generate revenue from advertisers to support the whole thing. In short, the advertisers fund Firefox and Chrome which hire people to program the thing, who are then supported by consumers. Google ‘taxes’ on the process of communication between consumers and producers, which is also the reason why it has grown so large. The global wealth is concentrated to them, and this is afforded by the humanity. What if consumers, the developers among them, directly run an opensource project ? Isn’t this way more efficient ? Therefore, the assumption that advertising economy supports opensource is plain false. Moreover, we shouldn’t help such economy grow, as how Mozilla helped Google remain monopoly. The cost of developing opensource isn’t really unaffordable. Rather, the main problem is to avoid duplicating projects and better organize developers, with this project. The main cost of The Network is the development of the core, maintenance of the code-base, and management of sites. What don’t have is the cost of running servers, keeping domains and advertising. The Network is the thing that organizes people itself, so it will. Dweb economyThe main economies: Content, which users produce and consume Platform, where users communicate Service, offered by machine, eg. FileCoin. If users tolerate advertisements which cost users’ time and attention, then they are probably willing to donate the same value which advertisements could have cost them. Proven by centralized sites already, such as bilibili, users are only not accustomed to donating. They are just not aware that ads actually cost them more even though donating seems more costly. Online shopping platforms in China usually do a ‘hidden’ minor donation each time people check out. Are people annoyed by this ? They aren’t, but they don’t want to donate when it’s not convenient. Patreon and paypal are common services for donation on centralized web, both of which extract a certain percentage for transaction fee, which effectively stops this economy from growing. How could donation work with these monopolies ? The cost could be lower with blockchains, minimizing transaction fees, which is already widely deployed but not actively used. How shall we distribute the donations received ? The conventional way is to let the users choose, which is greatly affected by the exposure. Softwares or sites users directly use will possibly receive more donations, but even in centralized web, the sites and softwares have a lot of dependencies. Commercial softwares are paid by their users, and the commercial components are then paid by the company, which is a market where the developers set prices. Such pattern won’t work since we are all opensourced and we can’t force users to pay, which is different from the relationship between companies that is enforced by the copyright law which has little ability here. Open-source needs a new way of commerce. Many well-known projects are over-funded, while new projects are stuck for the lack of attention. Ad-block is an example, which is probably the project that is even known by every internet user. Reportedly it has received millions of dollars from users and corporation partners in Acceptable Ads program, for its extraordinary attention, with little developing efforts. Arguably, ad-blocking is almost a kind of the simplest technologies, which just filters elements on a page with a blacklist. Do they really deserve that amount of money ? Should projects that more people use just get more money ? The two questions also apply to commercial products. No matter what they feel, the money should fund other projects that do matter and lack money. If the fund is adequate to free some developers some time that is sufficient for the project, I think, it’s enough. In conclusion, current donation model is distribution according to ‘attention’ which means the more exposed projects usually get more fund. To change this, we can change the exposure and redirect the attention of users on purpose, for example, by re-designing donation system and interface to automatically allocate some fund for the projects behind the application being donated, so distribution will better correspond to the distribution of one’s performance.","categories":[],"tags":[{"name":"draft","slug":"draft","permalink":"https://planetoryd.github.io/tags/draft/"},{"name":"dweb","slug":"dweb","permalink":"https://planetoryd.github.io/tags/dweb/"}]},{"title":"Harmony of dweb","slug":"harmony","date":"2021-02-20T06:32:04.000Z","updated":"2021-03-18T12:35:42.363Z","comments":true,"path":"2021/02/20/harmony/","link":"","permalink":"https://planetoryd.github.io/2021/02/20/harmony/","excerpt":"","text":"We haven’t associated ‘harmony’ with centralized web for a long time, and this is right, for there are various forms of spyware, malware and viruses. One of decentralized networks, ZeroNet, is named so probably because it aimed to start from zero where the centralized originates as well. Though it was indeed undeveloped, the developers and users supported the network and funded the project. The author of the network does have received much funds, but the developers of other sites didn’t. It is all about contribution and gratitude. The discussion sites on it don’t need much maintenance since the users generally appreciate the network, while the moderators of centralized sites are getting exhausted clearing the spammers. When you are trying to get some software, the malwares are downloaded unexpectedly. Then, how is the centralized web harmonious ? Is it natural to be like this, and even no one has questioned it ? The reason behind is that the centralized web lacks trust, even though everyone isn’t necessarily anonymous and is constantly being monitored by internet monoplies. They can’t trust each other as well, for everything is actually done by the platforms, so the users can only trust platforms which are not trustworthy at all. Does harmony really matter ? It seems harmony is only the relationships between users, and we can just go head and use AI to filter out spammers which does work to some extent. It is about the development of internet, however. Opensource develops because of harmony, and today it has achieved many great projects. Harmony isn’t simply about the spamming. ‘Modern’ websites have been trying to establish relationships between users, which is called user-stickiness/reliance(用户粘性). It is a great way of getting users stuck in their platforms and further reduce the chance of other competitors, and users are more well-bahaved. The harmony does not happen, however, since the establishment of relationships is only a strategy of the platform. The stickiness comes from the need of communicating with friends, which has nothing to do with the app. They can communicate if they use another app too. Except for the friends, what can happen between two complete strangers on some website, and does it matter to the management or desicision making of the website. As a result, the spammers appear and the attackers are constantly flooding websites. They users don’t really have true relationships on the centralized web, which are insignificant to the site owners. In a dweb, all people share resources, files, and some of them are site runners. They trust each other and have relationships as long as they use the network, which happens because they share things, and the trust appears during their mutual help. The trust also happens on another layer, where projects, sites are funded. Then they don’t trust the owners of a centralized site, but directly help each other instead, which is exactly how dweb works. Dweb won’t work without the spirit of helping. Typically, users are semi-anonymous, so the relationships appear, but the centralized platforms greatly interfere this process. Dweb is a network of users’ very own, where they are no longer customers of some platform, but run the network for themselves. We see all sorts of internet bureaucracy, from the simple hierarchy of a small forum where the site owner has the ultimate power and he gives the permission to different levels of moderators, to the more modern one of some larger platforms where the power is more distributed and the procedure is better designed. The phenomenon seems to be very natural, since the reality world always has governments which represent the people to some extent. The very feature of internet actually makes anarchism possible, which is exactly a purpose of decentralization. If one small group of people have got the power, then they have the chance of violating the demand of people. An argument is to rely on the competition of platforms, so that a bad-bahaving platform will be replaced soon after they have did something critical. The compeitition doesn’t work, as the platform will only be replaced when it is really unbearable for most of its users. Even in dweb, it is still possible that the site owner with much power would do something similar. Therefore, it is never a solution to have a small representation group of people with great power. How the people govern themselves ? The answers differ depending on how we define the goal of governing and what sites should be like. If a website is to enable users to host files, then most current dweb are capable of that, which is mostly done by the algorithm itself without any human operation. To this definition and goal, we have already done the anrachism of a cloud hosting website, like Google cloud or 百度云(Baidu cloud) in China. Then the question is, how is the bureaucracy of such kind of websites necessary ? This is the first kind of site where we have achieved anarchism, with living examples. Websites of the second type have to deal with different opinions, the centralized kind of which uses its power to enforce that only one opinion is adopted. The opinion can be, for example, what should be shown as the best article on a platform, what topic should be displayed at the top, or whether some content is allowed on a site. Sites of the first kind are not the second kind becase of the hash algorithm which maps the hash with the corresponding content exactly, so that there’s absolutely no disputes on this, which is a network-wide consensus. For the second type, we have several choices offered by the dweb, to turn to the subjective WoT, or the group of elected moderators, or even the mix of the two where both strategies have a certain influence on the final decision. The subjective WoT means that everyone has one’s own distinctive network of friends, which furthers the idea of people governing themselves to friends governing themselves for a goal, which is obviously harmonious.","categories":[],"tags":[{"name":"draft","slug":"draft","permalink":"https://planetoryd.github.io/tags/draft/"},{"name":"dweb","slug":"dweb","permalink":"https://planetoryd.github.io/tags/dweb/"}]},{"title":"Dweb in a nutshell","slug":"introduction","date":"2021-02-20T06:32:04.000Z","updated":"2021-03-18T14:47:02.973Z","comments":true,"path":"2021/02/20/introduction/","link":"","permalink":"https://planetoryd.github.io/2021/02/20/introduction/","excerpt":"","text":"simple intro written for some of my readers Dweb is a proper noun that means ‘decentralized web’. The current internet structure is centralized, which means the corporations run computing centers with large upload bandwidth and people generally get data only from these data centers. The direct consequences are that the privacy of users is constantly being robbed and monetized, as the global capitalism develops, which is caused by the centralization of influence since some application or website is used by plenty of users. They can use the influence to do anything, including putting ads anywhere, getting revenue as a middle man, and even affect the politics. The centralized model also makes it easy for censorship. For example the GFW can just use a blacklist to block any unwanted entities. In terms of technical efficiency, the competition and protocol barriers between companies block any possibility of reusing codebase and data which will happen in dweb. Most sites are similar at some extent, for example, video sites, social media and instant messengers, which all have the same content hosted multiple times and similar technical architecture. The three categories can have a common template that other new sites can be created from, which is the key of reducing the cost of introducing new competitors that benefit users, which is possible on dweb. The dweb connects users instead of companies, which reduces the intermediaries such as an instant messenger between users. If people originally download content from the data centers, then now they download from each other, by uploading to others. Such model doesn’t produce unnecessarily large monopolies. If the traditional web is considered to have reduced the intermediaries between customers and producers, from like three agents to one platform, the dweb directly reduces it to zero, by user autonomy with rules executed by the algorithm. The count of users is far greater than the servers corporations have, which means we have more IPs. GFW doesn’t want to interfere the international commerce, so it tries to distinguish the ‘illegal’ traffic from ‘legal’ traffic. However, we will always have more developers than GFW, so we can disguise the traffic to make it look more ‘legal’. There are many types of dweb applications, as shown (classified by purpose), including the most well-known one, blockchain which has the ability to reach a consensus over the network. Even blockchain itself still relies on other types of decentralized networks which are hence crucial and fundamental, for example, the anonymity-oriented crypto-currency Monero needs I2P for transport-level anonymity, and Filecoin, a market that trades the service of storing and transmitting data, heavily depends on a decent data exchange protocol. Content-addressing: Addressing content by its hash, instead of location（以哈希编址，而非存储位置） The history of dweb dates back to 2000, when Gnutella was firstly introduced. Then in 2001, the concept of content-addressing was implemented by BitTorrent, which developed DHT algorithm four years later. These are the fundamental algorithms for the later networks which have more innovations in other aspects. The latest technologies in this field are IPFS and ZeroNet, respectively featuring modularity and diverse media, which is also where the idea of The Network comes from. However, in my opinion, they all have failed to replace the centralized web. Imagine you are going to deploy a server for your personal blog, and you are looking for such service with Google. Firstly, you are motivated enough to use Google and take some minutes to find a provider and take another tens of minutes to register an account and make location choices. Secondly, you won’t reach the optimal choice, as the search results are biased and you don’t have the effort to check every provider. This is what is happening right now. FileCoin, as one of the diverse applications of dweb, solves this problem by protocol and automation. Each service provider will publish an record on Filecoin blockchain, by which you will find them. You buy some coins and ask it to serve your website, which will automatically look up the records and send them your data to multiple service providers. Filecoin will regularly ask the providers to prove they are serving your files, through some algorithm, and they pay them. Visitors of your blog will view it on Filecoin, which Filecoin also ensures to be as fast as possible by automatically checking the speed of candidates and selecting the optimal.","categories":[],"tags":[{"name":"dweb","slug":"dweb","permalink":"https://planetoryd.github.io/tags/dweb/"}]},{"title":"对于做笔记方式的思考","slug":"note-taking","date":"2021-02-20T06:32:04.000Z","updated":"2021-03-18T12:35:30.599Z","comments":true,"path":"2021/02/20/note-taking/","link":"","permalink":"https://planetoryd.github.io/2021/02/20/note-taking/","excerpt":"","text":"传统的方式是用手写，因为相关规定，学校里也只能用手写。新的笔记方式也很多，在电脑上做，手机上，用不同的形式，作图，简图，思维导图，输入的方式也不同，文字输入，数位板输入。然而我已经试过上面所有方法，没有一个是满意的。手写是我最早用来做笔记的方法，也是现在选择的方法。笔记不是写文章，文章是给别人看的，笔记是给自己看的，目的不同，需求也不同。手写最大的问题是不能修改，修改之后会造成内容的混乱，看着也不美观。键盘输入又有不方便，不便携，组织文本耗费时间的问题。若是要作图，可以考虑用鼠标，但这完全丧失了意义，笔记是讲究效率的，不能占用很多时间，于是很多人有选择用数位板，但数位板作起来和手绘也没有多大的进步，它只能擦除，而不能自动生成一种笔记的形式。 笔记应当是对大脑记忆的补充，自然不是给别人看的。相比文章的顺序结构，大脑记忆是网状的，甚至有时是零散的。因此不能强行用一张思维导图做笔记，也不能完全依靠文本，更不能只画一张图，流于形式。笔记有时只是唤醒记忆的方式，为了读写笔记的效率，某些文章具有的特点是可以放弃的，比如笔记不需要很完整的论证过程，不需要记录全部的内容，因此笔记是不能像文章一样，大段大段的，更不是写一本书，结构那么完整（所以说某些教辅书籍非要把书做成笔记的形式，用手写的字体，就很怪异）。理想的笔记形式应当是介于结构化的图示和文本之间的，也就是思维导图和文本的穿插。人脑的记忆中有很多关联，知识都是互相关联的。手写的笔记完全就不能产生关联的效果，譬如看到一个记录的知识点，想立刻看到其中引用的另一个知识点就极其困难，笔记的效率从何体现。 已有的软件没有一个能满足以上要求，连我都找不到，因此考虑有空编写一个。但最重要的还是对笔记方式的思考，现有的软件大多没有任何创新，形式相同，便利性也没有多大提升，没人对笔记方式有任何新的思想。新的笔记软件，首先大致的笔记结构应当是一张知识网，并且允许多个根节点，即一些有结构联系紧密的知识，和一些其他零散的知识，知识可以是任何形式的，文字图片之类，知识的联系可以有各种不同种的，直接引用、子集关系、其他任何关系。笔记的显示形式也一定不是唯一的，不像一篇文章只能顺着读，而不能倒着读，笔记可以以任何顺序阅读，软件应当能根据阅读者的需求以任何视图展示笔记，查询一个知识点，会顺着知识网查询相关的知识，并根据联系筛选。但这样的软件一定是不需要把所以知识网放在一张图里的，这样的功能是没有实际意义的。知识多了，放在一张图里过于密集，不能真正地有效地复习，这是我看到某些软件有的不必要的功能。笔记的显示形式也不能是思维导图的形式，还是类似的原因，人的阅读方式是顺序的，不是网状的，网状的性质是知识的内在组织方式，而非人能接受的形式，尤其是一大张思维导图，让人头目眩晕，根本没法阅读，也不方便显示细节。显示形式应当是以顺序为主，网状为辅，网过大的时候就拆成几个，顺序排列。软件也要能按需隐藏细节，这又和百科的形式不同，百科的细节是很多的，但作为笔记就不行了，说是“按需”，说明有时候也是想看一下细节的，手写笔记就不能做到这点。 笔记更重要的是做笔记的便利性，像那些美化笔记的，甚至用各种颜色写得花花绿绿的，如果不考虑对可读性的一点点提升，可以说是得不偿失了。手绘思维导图，或者鼠标拖拉绘制图示，也是很麻烦。那么新的笔记软件应当如何改进，其实核心思想是替代用户完成一些枯燥繁琐的事务，也就需要预测用户的行为，比如我输入”y=”的时候能自动弹出”f(x)”，和一些其他的选项，对于关键词要在已知知识结构的情况下自动加粗。图表的绘制一定不能是用鼠标或者数位板，而是要文本形式输入，并自动生成 。还有一大功能是对某个书籍做笔记，做在纸质的书上，编辑pdf，或者做在另一个本子上都很不方便。实现这个功能并不复杂，只要添加一个阅读书籍的视图，和引用书籍的功能即可。 后续考虑得出，这个阅读笔记一体的软件应当排在分布式网络之后开发","categories":[],"tags":[{"name":"draft","slug":"draft","permalink":"https://planetoryd.github.io/tags/draft/"},{"name":"dweb","slug":"dweb","permalink":"https://planetoryd.github.io/tags/dweb/"}]},{"title":"Problems with blockchain and WoT","slug":"blockchain-and-wot","date":"2021-02-20T06:32:04.000Z","updated":"2021-03-18T12:36:24.244Z","comments":true,"path":"2021/02/20/blockchain-and-wot/","link":"","permalink":"https://planetoryd.github.io/2021/02/20/blockchain-and-wot/","excerpt":"","text":"The inherent centralization of blockchain Sybil attack: basically, the attack that tries to break a reputation system with fake identities. Blockchain is known by its trustless pattern. To prevent some common problems, like sybil attck, they use ‘value’ or stake as the way to keep the system working. Whether some coin uses proof of work, or proof of stake, one’s influence on the network is always proportional to his wealthiness. The word ‘influence’ stands for the probability someone is chosen to add blocks. The rich, as a whole, usually have more probability than attackers overall. PoW uses a method of hash brute forcing to allocate probability of being permitted to add a new block to the chain, among miners. The more hashing power, the richer a miner is, more precisely, the more money thrown into mining. There is no significant difference between PoW and PoS, except that PoW consumes more energy and distributes wealth more easily when starting a coin. Mining on PoW, the miners burn the money, rather than stake like PoS, because hashing always costs money. Supposing hashing doesn’t cost money, it has no effect, obviously. As a result, the network naturally centralizes. The validators that maintain the network are the minority, while the system can’t sustain without them. The special characteristic of PoW is that it burns real money, in contrast to proof of burn. IOTA, which claims to be decentralized, uses a DAG instead of a blockchain. It requires every node to validate other two transactions when doing a transaction. Actually I think IOTA is a scam, and is vulnerable to sybil attack, as it has a coordinator node, ie. single point of failure. I also have a similar idea of using a DAG. They introduced PoW to solve those problems, which I think is non-sense. A feasible way is probably to associate the weight of a node in a DAG with the wealthiness of the transaction sender. Thus, the system sustains itself trustlessly, as the rich always have more influence and are willing to keep the existing order, the distribution of wealth. So, this approach still favors the rich, centralizing the network, which is meaningless for me. Is there any way without letting the rich have more influence to build a crypto-currency ? I don’t know, but surely without such a system dominated by the rich, no one would agree to keep that distribution of value, or wealth. You may argue that blockchain isn’t necessarily to be used as some crypto-currency. The fact is that there’s no way to reach a consensus without such a capitalistic algorithm, in a decentralized way. As I’ve said above, PoW is equivalent to PoS, and PoA isn’t decentralized in the first place. However, blockchain isn’t the only option towards a decentralized network. Years before blockchain, there had already been many networks for sharing files, some of which implemented Web of Trust to prevent spam. Web of trust is also used in GPG, with tens of thousands of users. Trust is a natural way how people organize together, since the appearance of human being. Accordingly, blockchain is capitalistic, which establishes consensus via private ownership. It is like a coincidence how these networks organize like a real world society. The nature of WoT is that everyone has the freedom to trust others at one’s own will, which is truly decentralized. Moreover, consensus, in the a broader sense, is not achievable for people having different belief. WoT respects the freedom of not achieving a consensus, for anyone. The ‘consensus’ here means the single source of truth. It can be the distribution of wealth, the allocation of domain names, and so on. That’s the consensus. I agree with the idea of ‘blocklist’ in ZeroNet. A blocklist is subjective. Nobody’s freedom or opinion is ever compromised. A notable difference is that trust is a kind of value that is not tradable. Tradability creates unequal value distribution. Unequal distribution gives us the opportunity to establish consensus. Trust is static and subjective, implying it is naturally censorship-resistant and scalable. Nothing would happen even if half of the network is banned, which is common in some censored countries. The same thing would cause a hard fork on a blockchain. I didn’t say blockchain is bad, but undoubtedly blockchain is full of scams and even claimed to be so-called ‘Web3.0’. That’s just hilarious, for some capitalists and even some scammers among them to be the inventor of that thing. They also set up some DNS services like ‘Unstoppable Domain’ and ENS. Imagine how they sit at home, collecting money every time a new domain is sold. Decentralized web should be censorship-resistant, first of all, thus isn’t a sub-project of Ethereum or anything else. Does blockchain really incentivize creations Steemit is a blockchain that rewards writers based on readers’ responses, which is often seen as a new way of ‘incentivizing creation’. Steemit is a typical example among such blockchains, which rewards the writers every time they got a like. As everyone knows, the coin market has to have people buying it to have value. They faced some financial difficulties as written in their wikipedia page, which is what I expected before reading. To make the whole ‘incentivizing’ system work, they have several choices: do nothing and wait the capital to run out, start a Ponzi scheme, or let the loser writers to pay the winners who got more likes, or the most common one, run ads just like what centralized web does. They have probably chosen the last two options. The former is a zero-sum game, which doesn’t generate any value unless the readers pay. Similarly, the latter is actually paying by the advertisement companies, which makes no difference from large corporations such as Google. As we know, the ad-driven business is broken, so the whole blockchain becomes nonsensical. Donations ? But, why donations with extra steps ? The business model driven by donations does work, which is also what I recommend to do instead of running such a blockchain. Even if they set a paywall, the quality will be soon shared on another decentralized network, inevitably. This is why I didn’t mention the method of paying by readers, because it doesn’t work, even without a dweb. Therefore they didn’t use a paywall, and the sole purpose of the blockchain is all about paying with a bunch of complicated steps, redistributing a part of the money to the writers, another part of money to the investors, and the final portion becoming the revenue of the Steemit corporation. None of the ways runs the so-called economy (except for donation which doesn’t need a monopolized blockchain payment method). The blockchain only blinds your eyes and cuts down the rewards of writers. What shall we do with blockchain ? Blockchain does create an economy if used properly. The works of writers can be copied, and should be copied, which is not a proper target of incentivizing. However, there’re things that can’t be copied, such as transfering data, FileCoin. Those are mostly services, where blockchain really suits. Other tasks like searching have already been tried and implemented with Web of Trust in last 20 years. WoT is more decentralized and more tolerant to network failures, as I wrote before, but it is relatively small-scale and community-driven. Today, things powered by money run faster. According to my observation, there is a such blockchain. I believe it won’t work currently, since they don’t even have a dweb with enough content. Co-existence of blockchain and WoTThe network can’t be totally trustless or moneyless; it has to be like a reality society. Trust is what we get from near friends, which is long-term and limited to a few people. Every individual is responsible for its own ledger of trust, while a blockchain is a global shared ledger. People do need some group to represent the will of the majority, in contrast to the anarchy of a pure WoT. It is also similar to the governance of a site which has a group of people owning and maintaining the site. The reason why I was against blockchain is that it was overrated. When the management of a site goes off, we can simply purge that site and adopt one of its competitors. There’s no cost in setting up a site and advertising for it in dweb. The recommending algorithm naturally promotes the truly quality content, or sites. Finally, we don’t have scaling issues whenever a site goes viral, even without extra maintenance. The competing environment makes such “centralization” acceptable. However, the reality is that many users and investors are still undereducated, fooled by fancy whitepapers from blockchain companies, which isn’t a good atmosphere where the blockchains compete for governance. In the future, the project will probably take over blockchains in most fields where we don’t need a global consensus, destroying the dominance of dweb by a single blockchain. Those blockchains aren’t really needed by users, but they hyped and created that non-existent demand. They do know the demand doesn’t exist and the users are tricked, so their sole purpose is to get money out of the market sooner or later, which is the bad competition environment I said. Solutions without a blockchain are generally more efficient, for like a decentralized forum, social network, and even name service, as they don’t have to get a consensus. As developers in this field, they do know these solutions. For the sake of profit, they “discover” every possible area that might be related to blockchain and create nonsensical blockchains, along with fancy websites and shiny whitepapers. The evolution of trust What is the fundamental difference of the trust-based decentralized web ? Today, before our dweb have been developed, we still use those services from large corporations, Google and Baidu, playing the most significant roles in the internet of the world and within China. Both have been criticized a lot till now, and kept going well with their business for they have already monopolized the market. After some incidents, a few started to use alternatives, which usually lasted only for a couple of months, and the majority forgot what the corporations have done to them. The logic behind the scene is simple, few alternative search engine survive for the lack of capital as the advertisers tend to cooperate with those existing winners. Baidu is actually worse than Google. Its cloud storage service (Baiduyun) was thought to be overpriced and moreover they have attracted many people to upload their own files, but afterwards, they are required to purchase premium plan, or they will be throttled to a unreasonably low download speed. The anger of people soon emerged and lasted for a long time. They didn’t do anything until the public opinion heated again and again. In the end, the speed limit is raised from 2KB/s to 100KB/s, which almost succeeded in silencing the majority. The majority has no weapon to defend unless the situation is critical, their files are almost hijacked, and they have to use it for other people, relatives, friends, and the company they work for use it. The better one, Google, is actually bad for the adoption of their future competitors, the dwebs, on the other hand. The cost to use an alternative is too high, and the cost and risk of running an alternative sustainably is not affordable for the investors. The true demand of people is the files on the cloud which the company has successfully monopolized. Under such circumstance, a new company who wants to challenge Baidu in the field of cloud storage has firstly to get enormous capital which is hardly possible， secondly to advertise which is even more unlikely since Baidu has no reason to support its competitor. Many brands are trying to offer cloud services along with their hardware products. This works, but only in a small scale, unless they can conquer the market like how Baidu did in a short time. Baidu did that by asking the government to ban Google. What does ‘trust’ mean here ? ‘Trust’ means to trust something and start using it, as a process. The process of ‘trust’ also comes with risk and cost which includes the loss of the chance to use other products and resource and the convenience of migrating to another product. When you start to use Google, you trust it, and the cost of trusting that is almost zero for it is shipped as the default search engine together with nearly every device. The time you realizes the flaws of Google and tries to find a new alternative on it, the search results will soon cool you down, because it takes time to check every new search engine and you are also anxious whether it will last or just disappear days later. In the end, the users are simply afraid of trusting new things. An easy solution is to have a federated architecture to solve such trust issue, lowering the cost of deploying and adopting new entities of some service, and the cost of being trusted. Typical examples are Mastodon, and Matrix network. The networks are formed through the connections of users, and the data is exchanged within the network of friends. From the perspective of result, they haven’t gained sufficient users they deserve. Because they can’t compete with the centralized monopolies without overthrowing the broken advertisement economy. The abolishment of ad economy can only take place when the network itself is appealing enough for the users, in the first place. Federation of applications is like running multiple Googles, and keep them in fair competition because we have an shared entrance where users can choose one of them freely. Each service provider in a federation follows the same protocol, so that the behavior and whether it acts maliciously can be automatically examined by the software which doesn’t cost users’ efforts. Hence the cost of trusting is reduced, with federation. When trusting takes near-zero cost, the competition mechanism truly works. In The Network, we have the WoT, which is further optimized with automated trust, requiring no user interaction. Therefore, the cost of trust in WoT is relatively low. The sites, and blockchains are the things that cost much to be replaced with alternatives. Although things should differ from the centralized web for the withdrawal of ad-economy, whenever a site’s management is corrupted or hacked, it takes time to move onto a new site. A solution is introduced for sites that are more community-based, which is to give the power to the WoT. As a result, the site is less dependent on the development team, and as robust as the WoT. The evolution of trust is about the three stages mentioned above, from high-cost to low-cost, decreasing the difficulty of purging bad actors in the competition. The cost of trust lowers, and the trust itself decentralizes. WoT is a ‘federation’ of users, and the entities are smaller. The services in such federation are all provided by users themselves, on each device of them. The difference is that WoT forms a network, while federation is only a list. A friend of your friend is probably also trustworthy, so ‘trust’ forms a network. Once again, it reduces the cost trusting, because we don’t need to trust more people manually since our friends have already done this for us. Blockchain is a special kind of thing. The blockchains themselves are the entities to be trusted, and the smart contracts on the chains inherit the trust. So, blockchains can be trustless inside. Services other than Google, such as DNS, the authority of naming, becomes the authority as we trust them. Every browser or operating system has a default DNS server, which is also a part of the global hierarchical DNS system. Supposing someone replaces the DNS server with his own server and tries to advertise for his server, a few people may try it, but soon the attempt of taking over the authority will fail. The cost of trusting and using the new system is too high, which is modifying the settings of DNS server, what normal people won’t bear the inconvenience to do. Also, people don’t expect others to do the same thing for the same reason. Such an authority factually monopolizes, since it has created enormous cost of trusting others. The cost means the total price we need to replace this monopoly, which is obviously hard. In contrast, an authority in a positive competition environment may perform better and is unlikely to be evil. An authority doesn’t necessarily monopoly, although most daily internet services are. In conclusion, the cost of trust is reduced with technologies by automation, through a better way of communication. Forking a siteMany sites are platforms, which have the characteristic of the heavy reliance on popularity. A platform site tends to form a monopoly after conquering the majority, and the competitors can’t bear the cost of changing users’ habit. Platforms, as the name implies, they hold items submitted by users which will then become their means of sustaining the monopoly. This is a kind of trust issue, because the migration of the items that have been submitted is part of the cost of trusting new entities. The monopoly has no excuse to give out their user data, which will never happen in the centralized web. How does the dweb solve this problem then ? We have the client for every user, and that’s how we enforce sites to share data through cross-site interaction which allows us to build a new UI for an existing site, a new system of content promotion, or a totally new site, etc., with original user data untouched. This completely destroys the barrier of competition that almost exists everywhere. Web3.0 What web1.0 and web2.0 are doesn’t matter, and web2.0 should be what we are using right now.The term web3 has been used and defined by many, such as blockchains, but here we are to redefine it. What should Web3.0 look like ? A market of data service, or a market platform of articles, or only blockchain itself with some shiny smart contracts ? The first item is FileCoin, a project based on IPFS. As said above, market, or blockchain causes centralization, which is not practical for a censorship-resistant network. Therefore, Web3.0 can’t completely rely on FileCoin, which proves FileCoin itself can’t be Web3.0. The second one is Steemit, a DPoS blockchain featuring its blogging platform, rewarding writers according to the upvotes from readers. The first thing I wondered was where the money came from. Most common business model is advertising, like what those big corporations do. Another tricky way is to let the speculators pay the bill, or to let the losers in the writing competition pay the winners. The former seems to work, as a Ponzi scheme, but it is uncertain to last long. The latter needs writers to ‘invest’ some money first, which is actually an option when you register an account on Steemit. Of course, you can also choose the manual review way, with some delay. There’s no other way to keep the whole thing work, thus they must be advertising, making no difference to those big corporations. The third is Ethereum. Let’s skip this, because, how can a blockchain be Web3.0. Capitalistic advertisingAdvertising, capitalistically, is evil. An extreme example is in China. PuTian hospital, a private hospital who invest massive capital into advertising with Baidu, another dominating Chinese search engine who sorts the search result by bidding. There’s no difference among those companies using advertising as their business model. Either show some annoying ads, or more implicitly, change the order of search result, favoring their sponsors. The whole system is capitalistic. The system inherently promotes those who are willing to spend money and more money on advertising, and drives out those who aren’t. Other side problems, tracking, surveillance, etc. automatically emerge under this system. You might argue that, the advertisement payers enable us to use services freely, and support those content producers. People aren’t satisfied to watch ads. How can it be a source of income for content producers. There shouldn’t be ads in the first place. The more money an advertiser invest, the lower the product quality is, and you don’t find a good solution when you search something, either. There can be good advertising under WoT with zero-cost. Domains Domain name, and this also applies to other names, like user name. DNS is the centralization of naming, which is controlled by an organization. Each site has to afford the cost of maintaining one or more domains every year, and they have no choice for the absolute domination of DNS. People also buy domains that they don’t really need, and speculate whenever others truly need them. Is this a good system ? Shall we sell domains ? Who should own some domains ? Instead of selling domains to who have the most money which supports the capitalistic advertising, the dweb will allocate domain names to those who need it, based on WoT, which means that if your friends and friends of friends trust you and believe you deserve the domain names, you will simply get them. The names should be given by its users themselves, but not by how much money and value the name holders directly produce. The blockchain name providers are like the DNS, acting as an authority, which has no difference from the centralized naming service. Therefore, blockchains in terms of naming can’t be the Web3. ConclusionThe real world have been controlled by capitalists, the top elites. The internet should belong to its users, the proletarian. Whether for communist countries or western countries, the unbiased freedom of speech is always a driving force of innovation, and reformation of the society. The ranking algorithm can be biased by the rich with enough voting power, fundamentally. A blockchain has no way but to favor the rich in every aspect, including content ranking, or it would be vulnerable to sybil attack. We must seize the means of communication—— Edward Snowden","categories":[],"tags":[{"name":"draft","slug":"draft","permalink":"https://planetoryd.github.io/tags/draft/"},{"name":"trust","slug":"trust","permalink":"https://planetoryd.github.io/tags/trust/"},{"name":"blockchain","slug":"blockchain","permalink":"https://planetoryd.github.io/tags/blockchain/"}]},{"title":"关于中文互联网的随想","slug":"random","date":"2021-02-20T06:32:04.000Z","updated":"2021-04-03T11:25:23.310Z","comments":true,"path":"2021/02/20/random/","link":"","permalink":"https://planetoryd.github.io/2021/02/20/random/","excerpt":"","text":"中文互联网 与中国国情有关的一些想法 从一种直观的角度来看，中心化互联网是碎片化、低质、封闭的，相反，去中心化互联网得益于结构和协议优势能克服以上问题。碎片化意味着中心化互联网不够互联，这个问题参见data structure部分，并且与其封闭性常常是同时显现的，但二者也有区别，封闭性和中心化网络协议问题是碎片化的原因，而人又倾向于寻找有关联的内容，这进一步促进垄断，促进封闭性（如搜索引擎）。低质是由广告经济直接导致的，在中文互联网表现较为明显，而在西方发达国家，巨头公司垄断更加全面，因此能有一定的“改革”的余地（谷歌搜索结果相对百度更好）。 在这个广告经济时代，越来越多的人的时间被这种互联网消耗，包括初中生、小学生。社会压力的增大、家长教育时间的减少，于是学生的许多时间就花在手机上。这种量的学习本身是违背人的天性的，不能苛求每个人的自制力都很高。然而，摄取琐碎的内容只需要下滑一拉就行，但想去找一点有价值的内容，可能要去书店、图书馆、或者在网上找个半个小时。除非执行力极强，谁又能克服重重阻力避开这种互联网。国家采取一种中考筛选一半人的方法，这一半人上了高中，另一半人就荒废了。据我近几天的偶然观察，微信有个所谓的青少年模式，能过滤各种垃圾内容。“微信舍不得放弃青少年的流量，否则实名制开启青少年模式岂不是很容易”，这种说法我很认同。广告公司始终脱离不了它的本性，在接下来的十年中，中心化互联网的情况大概是不会好转的，人民浪费在上面的时间也大约不会减少。人常常把学生不学习归咎于互联网、手机，却忽视客观条件，获取一般内容远比获得学习资源容易。 2012年中国全国人大常委会通过了《全国人大常委会关于加强网络信息保护的决定》^[4]^。该决定要求有限度的网络实名，即服务提供商在“为用户办理网站接入服务，办理固定电话、移动电话等入网手续，或者为用户提供信息发布服务”时，要求用户提供身份信息。该决定同时要求服务提供者保护个人隐私和个人信息。2012年3月16日，新浪、搜狐、网易和腾讯微博共同正式实行微博实名制。 2016年11月7日通过的《中华人民共和国网络安全法》第二十四条规定，“网络运营者为用户办理网络接入、域名注册服务，办理固定电话、移动电话等入网手续，或者为用户提供信息发布、即时通讯等服务，在与用户签订协议或者确认提供服务时，应当要求用户提供真实身份信息。用户不提供真实身份信息的，网络运营者不得为其提供相关服务。”这是中国大陆首次以法律形式明确网络实名制^[5]^。 2017年8月25日，国家互联网信息办公室公布《互联网跟帖评论服务管理规定》，自2017年10月1日起施行。规定包括网站不得向未实名认证的用户提供跟帖评论服务、新闻信息的用户评论应先审核后发布等^[6]^^[7]^^[8]^。 实名制，是阻碍互联网发展的一大毒瘤。几乎所有在中国的网站都会要求手机号注册，也就等同于实名了。实名意味着把自己的虚拟身份与政府所知道的现实身份联系起来，互联网的匿名性基本上就丧失了。我所说的分布式互联网，其自由的特点很大程度上来源于它的半匿名性（semi-anonymity）。实名导致互联网处于政府的监管影响下，而这类法规总是落后于现实中技术的发展的，用落后的东西管理先进的东西，于是互联网就停滞不前，比如夺取通讯资料的活动就无法进行了（事实上这种管制还是可以靠技术突破的）。匿名性的缺乏干扰真理的传播，学者发出言论时，不能只顾及真理，他们还顾及利益集团对他们的影响。他们不能不顾一切的传播真理，真理的传播会对他们自身造成影响，影响升迁，甚至会遭受牢狱之灾。实名制。隐私是匿名性的基础，当我谈及隐私的时候，不是担心隐私泄露之类的问题，而是说隐私权的侵犯会造成匿名性的丧失。 匿名性不是造成犯罪的原因。假如一个犯罪集团能在一间屋子里商讨犯罪计划，但政府不能窃听他们或者根本就不知道，那么在一个匿名的互联网上这样做是没有区别的。政府不能因为罪犯能在一间屋子里交流而在每个人的家里安装一个窃听器，这和实名制是一个道理。这样的类比可能程度不一样，但性质是一样的。匿名性也不是可以取舍的，政府不能今天说匿名性是合法的，明天是非法的。因为匿名性是技术实现的，能靠政策阻止的匿名性都是不可靠的。正如政府不能在一公里外穿透墙壁窃听一样，或者说在几百年前，窃听器根本没有，那么这样的匿名性就是客观存在的，不是政府能改变的。与匿名性密切相关的就是密码学，尤其是非对称加密，这种技术一旦出现，就不可能被政府封杀了。对于dweb造成的可能后果，最好的做法就是什么也不做。 记得几年前有一种很典型的现象，姑且称为广告经济的初级阶段。在百度上去找一本书，或者某些破解的软件，总能看到“某某下载站”，这些站有很多广告，但各种盗版书籍、软件之类的很多，因此给百度的钱多，排名靠前。相比之下，官网根本就没法找到。如果说这是初级阶段，那么现在中国的广告经济就属于中级阶段，西方发达国家的互联网就处于高级阶段。从初级阶段到中级阶段，很明显的体验就是广告变得隐晦了，从一开始的满屏的广告、弹窗广告、什么乱七八糟不合法规的广告也有，变成现在各大巨头平台上穿插在内容里的一些图片，稍不注意还会把广告当成文章。高级阶段的广告更进一步，甚至发展到”ethical ads”，不过所谓“有道德的广告”让打广告的不太满意，所以也没有推广开来。现在广告“体验”的提升，似乎缓和了用户和广告之间的矛盾，某些地方甚至达成一种默契，即用户自愿看广告以支持内容作者，以及所在平台。从自己的体验来看，中文互联网确实有打广告能力的进步，广告也没有以前那么难以忍受了，但这意味着广告经济就是互联网历史的终结了吗，互联网真的会止步于这一种中心化的形式吗。 更底层的分布式https://www.eff.org/pages/openwirelessorg 有一个的mesh网络手机，https://volkfi.com/faq 与我之前说的一样，mesh网络不适合中国国情。首先无线电就需要注册，又是要交钱，基本不可能形成规模，哪天政府认为传输的数据有嫌疑可能就直接拘留了。目前分布式更好的方式还是混淆（obfs）。 又一个例子：政府的网络审查斗争偶然查看CaO的词条： 网络 氧化钙的化学式CaO与网络用语“草”的汉语拼音相同，故常被网友用来调侃 这放在几年前，估计会被加入政府审查的关键字名单，在各大网站上被替换成**，但现在政府没有做这件事了，可能有关部门突然醒悟了，人民的语言是不断演变的，这类调侃的语言，政府是不会跟上它演变的节奏的，因为政府总是被动的，毕竟政府自己不会主动创造这类语言。为什么举这个例子呢，一个词语，在人没有赋予它含义之前，没有人知道它是“非法的”还是“合法的”，同样的，一次网络连接的建立，也没有人知道它是否合法，先姑且不谈语言的发出者和连接建立者双方的身份，对于这种情况，政府要么用白名单，要么用黑名单，在人说出每一句话之前人工审核，一定是人工审核，哪怕人工智能也不行，在说完这句话之后，还要继续追踪，观测语言含义的变化，等哪天这种语言有非法的含义，立刻审查封禁。显然两种方法都不大现实，于是政府也就放弃了。 一次网络连接传输的数据，甚至会比语言还更难判定合法与否。连接的数据可以加密，可以在不同终端之间交换、分包、传递。但目前的状况是，语言的演变比人民对审查的抗争更活跃，因为人人都能改变语言，而对抗审查的协议不是每个人都能创造的。于是我们使用的协议就较为单一，等哪天政府破解了抗审查的协议，才会有人再去发明新的协议，处于一种被动的状态。政府审查语言，与政府审查网络数据之间有极大的相似度，对于前者政府是失败的，对于后者政府却基本上成功了。本质上的原因是主动与被动的差别，想改变这个因素，一定要发动人民，组织人民，用分布式网络提升抗审查协议的研制效率。 把人工智能用于审查，本质还是被动的，况且有审查用的人工智能，就有与之对立的抗审查人工智能，二者分别是识别和伪装。 分布式与当前互联网的比较 数据复用 Dweb的数据复用以IPFS为代表，运用merkletree；传统互联网主要用CDN，应该有类似技术。 中心化CDN的竞争会阻碍更高效的数据复用，因为两家CDN之间的数据共享大概率是不如一家之内高效⇒市场上CDN减少，形成垄断局面⇒CDN服务质量（达到预期的概率）下降；Dweb采用信任网络，不存在垄断问题 计算、数据存储 中心化网络有所谓云计算；分布式网络靠客户端计算，或沿网络委托计算，或依靠算力市场 云计算对于要求即时性的应用不如在客户端作计算，毕竟网络不能保证稳定 云计算不如基于区块链的算力市场高效 Dweb的主要应用不是算力密集型的 用户关系 中心化网络靠低级手段（各种营销手段、含低级快乐的内容）吸引客户；Dweb没有营销目的 中心化网络的用户之间的关系建立在平台的基础上；Dweb的信任网络完全由用户构成 某些现有网站的问题互联网的的目的是沟通，找所求的人或者物，或者发布关于人或物的信息，所以互联网追求的是沟通的效率。 群组、IM，主要问题是错误的排序方式（Element.io） 现有方式以人数排序。也许最初作这个选择的人是以为人越多的群，沟通效率越高，现实中人多只会让聊天区饱和，不适合严肃的讨论。 人数很少的群自然不能算最高效的，再排除人数过多的群，于是人数不能作为排序的依据，但可以作为筛选的依据，选取合适的人数区间。 群组主要的排序依据应当是活跃程度，如果不依照这个因素，导致不活跃的群成为新用户搜索时的第一结果，根据从众心理，这个群会继续不活跃下去。 毕竟是IM，活跃度是第一要素。活跃度适当的IM，沟通效率是最高的。 社交网络缺少互动（Mastodon） Mastodon好像是以时间排序，官网还是什么地方甚至自作聪明说这种排序方式好。之后的分布式社交网络应当避免这种设计。 中国的线上图书馆 图书馆要求实名制注册、登录，一开始我认为是防止盗版，但想了想这个环节盗版是防不了的。根据政府网站一贯的传统，实名制的根本目的是为了限流，限制了DDOS，也限制了正常阅读的人。国内垄断互联网企业能设置防DDOS的系统，对于这样重要的国民知识来源，政府不能也做一些工作吗。似乎结论就到懒政结束了，但还不止，因为改变互联网的事情不是政府能办到的。即便政府设置好的各种图书馆之类的线上设施，不能改变流量经济生态系统，新的内容不能高效的产生，最终也不会有很好的效果。 分布式如何与传统网络不同 拒绝做种的一大意义 有人为投放广告付钱，必定是因为投放广告能带来回报，即便广告投放的目标用户中只有1%甚至更少的人会成为客户。广告的核心是曝光，这就是为什么各大品牌争相获取冠名权，一个品牌出来曝光，其他品牌由于竞争压力也会跟随，这就是注意力经济。人的注意力是有限的，但有能力的互联网企业总是有办法继续开发人的时间，最好都拿去卖了，各大企业竞争有限的用户时间，于是产生一些“精神鸦片”一类的产物消耗人的精力，再是各种无尽的通知和弹窗，垃圾短信。有一点要指出的是，并不是说法律规范之后，广告就是可以接受的了，因为广告本身就是在人正常阅读的过程当中的入侵物，低下的广告成功率牺牲了所有不成功宣传的客户的时间，这个时间的总和加起来不会是一个小数目。人工智能定位目标群体确实是个好方法，但越高的准确率意味着需要越多的数据，越多的数据意味着侵犯更多隐私。如果用户本身有购买需求，接受一则商品的成功率显然是高了不知多少倍了，这时大约完全是靠电商平台的排名了。再者，主动搜寻商品和被动接受商品，同样有人工智能的推荐算法，自然是主动的成功率更大。 中心化网络网络只有两种选项，一个是以国营企业运营互联网，于是内容不会以点击率论去存，另一个也就是当下世界范围都采用的市场经济模式。国营企业的运营模式是众所周知，相对市场而言地低效，不太现实。市场竞争却又会带来激烈的互联网企业生存压力，这个压力被转嫁到各个大小内容生产者身上，逆向淘汰出，在吸引相同流量下花最少精力作成的内容。如果有更好、更理想的内容，许多人读完这样产出的文章之后大约就会体验较差，但广告已经看到了，目的已经达到，文章质量如何又何妨。分布式网络其实就有一种表达长期意见的方式，即Seeding（做种），空间是有限的，必然要作出选择。做种的目的不是筛选出好的内容，而是筛掉不好的内容，也就是说空间通常是足够的，但撤销做种也总是需要的。这就是和中心化网络截然不同的做法，传统网络以点击率作为是否留存的标准，而分布式网络取决于做种的人数。当然，做种只是底线，用于筛选超过红线，即用户忍耐的底线，极度垃圾的内容，绝大多数情况还是依靠Dweb网站本身的排序机制，这不是本节的内容。 这可不可以用于没有WoT的情况下判断内容优劣呢？不行，因为优质内容不一定peer多，peer多的又不一定是优质内容 当然，在实际操作中，不会是用户直接取消做种，多数是以网站自带的筛选排序机制排除了劣质内容。在用户没有主动要求、设置、空间不足的情况下，也应当保存相应内容，毕竟信息是以多为好。 中心化网络需要用户的时刻只在于用户看到广告的时候，于是其他任何时间，网站与用户毫无关系。当用户还在网站门外之时，就把用户骗进来；当用户看完一篇内容之后，就以各种推荐之类的方式让用户再看到一些广告。网站与用户的微弱联系是仅仅由广告维系的，在此之外的，不过是为了改进用户看广告的体验罢了。也就是说，内容不是目的，广告才是。 以直接利益为目的而不以内容本身为目的的现象不止存在于互联网，书籍的市场，尤其是教辅类的书籍。依据我亲身体验，八成的教辅书有凑字现象，各种编排错误、上下文不对应更是不计其数。这大约是教辅书市场的高利润导致的。其实这种先付钱、再阅读的模式与广告经济本质上是相同的。我们不可能在买一本书之前就得出准确的结论，这本书好不好，否则既然已经看完了，大约也不需要购买了（尤其是针对习题类教辅书籍，你不可能做完了再买，也不会在书店或者在线预览停留过久）。读一本书，一开始觉得写得很好，但仔细深究，就发现很多问题。许多书就是表面很好，一遍不能读出来，更别说书店里稍微翻阅一点。一旦买下一本书，我们对这本书的经济来源再也没有什么影响力了，毕竟一本书不会再来买第二次（作者本身有名气的，排除在外，大部分这里讨论的书与作者声誉关系不大），这本书是否畅销也只由读者的第一印象决定，而不是深思熟虑的结果。如果确实仔细反复审读的一本书，恐怕不会买了，假如人人都读完书之后再买，再付钱，这样先付款、再阅读的模式也就失去了意义，毕竟这和捐款有什么区别。这样的议论可以推广到任何其他类似的模式，先付钱再使用的软件、视频课程、培训。我们寄希望于各种内容生产者本身的觉悟，但这改变不了“即时获利”的系统性问题。广告等同于即时获利。 广告经济的特点不只是产生低质内容，更是在资源有限的情况下，压缩高质内容的生存空间。所谓生存空间在这里不仅仅指的是之前的曝光率问题，而是缺乏产出。其根本原因是广告经济使得有限的资本流向低质内容，而且人的时间也有限，阅读高质内容的时间就缩减，与流量经济高速发展之前，高质内容得到的资本自然就减少。 被广告真的不行吗的确有一些需要主动推销的情况，例如一些商品增加产量可以降低成本，但这显然不是多数。人总体的需求是不常变化的，于是更多的情况是人主动寻找商品，不需要所谓“被广告”。就算是一些主动推销广告，也应当以一种不侵扰用户阅读的形式进行。 还有一种情况，人不知道自己有某个需求，而是看了广告之后才产生的需求。这样的需求大概率不是必要的需求，因为广告而增加不必要的需求，浪费资源。 内容的自由，从产出到消费内容（文化作品、代码）的生命周期有三个阶段，生产 - 传播 - 消费。开源、知识共享可以说是实现了生产阶段的自由，部分实现了消费阶段的自由。但两者没有一个能成功解决传播阶段的问题，于是自由软件的传播受到现有互联网的限制，知识共享也没有激起很大的热情、造成多少影响。到了消费阶段，可以视为读者在阅读时的体验、或者使用软件的体验，是否被需要先买书再阅读，又是否有被广告影响，安装软件有没有要求先付钱。开源让作者能在编码时不受版权的阻碍，这是一种自由；内容传播之时不被资本扭曲原本的理想的路径，是一种自由；消费时不被内容以外的事物干扰，又是一种自由。 其中传播的自由，也就是不含偏见的内容传播方式，需要一个很重要的特性。The dweb的站点运营时完全在用户的电脑上，所有的运行数据，过程对用户可见，而且代码也通常是开源的，这导致了站点从运行时的数据，到用户资源都是公开的。原本中心化网络上站点的数据库和用户资源都是公司最大的机密，到分布式网络上反而公开了。结果有两个方面，首先中心化网络上的一些算法不能用了，比如不能确定准确时间，不能验证用户的authenticity，好在这两个问题都有解决方案。这种特性的好处是中心化互联网公司所拥有的的一切引以为傲的垄断的资本都不复存在了。 复用、协议化、标准化 以前总听到有人说分布式网络开发难、不会有发展，现在若是仔细设计dweb，怕是中心化网络开发更麻烦些。 开源算是在代码层面上的复用，各种容器技术是位于运行时的层面，还有例如从CS架构到BS架构的变化。无论是开源还是容器技术，始终没有触及更加形而上的复用。比如说复用平台上的用户关系，这点其实垄断公司已经在逐步的做了，小的网站都不怎么需要注册，用谷歌、微信授权登录即可。这样虽然属于复用，副作用却是造成了进一步的垄断，因为复用的对象不是自由的，而是受第三方控制的。The dweb所做的事情类同，WoT其实是将站点本来会承载的用户关系完全移动到了更低的层面，统一处理。中心化网络应该也有merkle-tree的结构，因此IPLD的做法相对于把这个tree从CDN运行的层面转移到更低的层面。分布式网络之中semantic-linking也就是把原本中心化网络的超链接行为从内容之中分离出来，就像CSS一样，使形式独立于内容，于是我们就能避免当今互联网中的各种不完美。这样，运行在dweb的爬虫就不需要分析各种奇怪的链接，不需要从混乱的页面中提取文章的大小标题。 AdBlock与广告的现状之前所说的广告方式的演变很大程度上是归功于Adblock系列插件，表面上广告是少了很多，但这意味着广告模式逐渐走向成熟、隐蔽。就以搜索引擎举例，也就是典型的有序内容，以前好像有特别框出的广告一栏，这样不至于分辨不出广告和原有理想排序的差别。我们假设搜索引擎根据查询字符串得出原有理想排序，这应该是我们所需要的结果，但由于Adblock对明显广告的阻拦，搜索引擎不得已改变原有排序，实现广告主曝光的需求。于是Adblock能力越强，广告与真实内容就越相似，最终广告与内容合为一体。可以说Adblock是网民的一相情愿吧。 流量经济现状下的封闭内容生态虽说各种低质内容仍是流量经济的主流，对于高质量内容的需求是不变的。在这个背景下，这一类内容倾向于在封闭社区内发展。这种社区就和寻常网民的思想不一样，其中要么是内容创造者不求回报（否则就会退化为流量经济），又或者是通过捐赠或收费之类的方式形成稳定经济来源。这类小团体必然是存在的，我也有见到一些，但不足以举例。为什么这样的团体不能长久，以至于壮大呢？ 还是要回归内容的本质属性，各类盗版行为不是法律这类低效的手段能阻止的。当这样的群体大到一定程度，其中各种盗版行为就不能阻止了。于是原本是这样的网站逐渐扩张之后，必然被流量经济同化，因为这时他们没有选择，广告是回本最快、最稳妥的方式。于是我们不常见到这里描述的封闭内容生态。即便是保持小团体的模式，互联网依然不能发挥其最大价值，原因很明显，一定是互联了、人聚集在一处，整体效率和价值才得以提升。然而扩张又会产生上述问题，因此这条路是行不通的。 匿名性和PoWThe dweb的大多数关系都是基于信用的一种长久互利关系，这也导致大多数人会保持半匿名（pseudo-anonymous），而不是完全匿名。在没有建立信用关系之前，peers之间总是有一段时间是用于尝试。一些资源被用于尝试建立信用关系，比如要求新加入的人完成验证码，或者PoW。因为这类用于尝试的资源不设门槛，容易被DDoS，终是要一种快速而易于验证的方法提高尝试的成本，同时避免影响潜在被信任者。 非自由收费模式我们定义人的自由是不受干扰的使用软件、阅读内容，物的自由是内容不受其平台的限制、软件本身的功能不受其他事情的影响。例如书籍作为商品售卖，它的内容被限制为一种商品，而不是让内容保持为纯粹的信息。显然，为了让书籍能像商品一样售卖，必然要把书籍的信息印刷在纸上，于是信息被载体所限制，不能自由地传播以至于卖书不得利润。就算电子书也能限制在某些电子书平台之内，变成平台的财产，而不是自由的存在。人的自由是获取、阅读信息的自由，而物的自由最终也是以人的自由为目的。 这时讨论的物是信息，因为信息易于复制的特性。软件同样是易于复制的，也是属于信息，因此开源也算是信息自由的一部分。然而信息自然不能独立存在，信息的自由性全然是人的行为所达成的，如果人不去自由的分享信息，信息的自由不会存在。对于内容，信息自由可以是由the dweb实现，每个节点都付出一定算力和带宽分享信息，但这只是传播阶段，信息仍来源于人的创作。创作者不把信息分享到the dweb，传播无法开展，连信息都尚未产生，又如何达成信息自由。因此，信息自由不能脱离人的服务，人不顾回报的付出是信息自由的前提，并不是说信息被放在某一处就自由了。正如书籍一般，自由软件缺乏维护服务便不能正常使用（此时开发人员要求付费服务，而你没有付费），所谓自由也无从谈起。 当软件需要收费服务，一定要加入一个区分是否已缴费的功能，加上各种人为的限制，于是重要的功能不能纯粹的存在。自由软件如果要求收费服务，文档一定是不齐全的，因为开发者不会尽力去写，代码一定是不易懂的，否则收费服务会失去意义（收费服务指广泛地收费，而不是针对个别疑难杂症；不齐全是相对于理想的自由软件）。因此，各种打着开源旗号行使各种营销手段的，可能只是把开源当做一种宣传方式，或者一种标签罢了。 广告经济能起到一部分为内容提供资金的作用，这是事实，但正如电子书平台、纸质书籍、非自由软件一般，广告被强制植入于内容，内容为其广告的目的所限制，广告经济的真正问题在于它损害了内容自由，而不仅仅影响阅读体验。 自由收费模式，捐赠许多人就会说了，捐款不是可靠的经济来源，那不过是开源经济尚未充分发展形成的刻板印象而已。人能接受版权法，在普通人不受法律严格监控的情况下形成软件需要付费再使用的“常识”，同样人也能接受捐赠的形式。至于公司之类的实体，则需要消费者的监督，促进相应的捐款（这也是正在发生的）。 互联网需求有人可能为流量经济辩护说它只是满足人的需求，本身没有错，虽然说它看起来都是水文之类的低级而新奇的内容。然而人的需求不是一成不变的，常常是可以塑造的，如果这类浮躁的内容可以被轻易的获得，那么人大概会多接触一些这样的内容，少接触一些其他内容，甚至阅读兴趣都会改变。同样，我们可以改变整个内容生态体系，减少低级内容，这并不会导致用户流失之类的事情，因为对于高质量内容的需求不是不存在，而是被掩盖了而已。 信任成本首先，信任不是单纯的念想，而是用行为表现的。不用行为表现的信任是没有意义的。因此这里定义，信任是相信某方并使用该网站或者购买该商品。流量经济体系下的信任成本是极高的，对于大多数用户，选择已有产品的竞争对手是需要作出信任抉择的时刻。在这种情形下，用户必然是希望替代品能显著地优于原先的网站，而这个优劣的批判很大程度上取决于替代品网站的用户数量。用户数量又直接决定网站的价值，也就是传统互联网的自然垄断效应，这里不赘述。也就是说，只有所有原网站的用户突然开窍转而用其竞争对手，才能达到用户的期望，这种情形发生的难度等同于信任的成本。 一个降低流量经济中信任成本的尝试：Alternativeto，这个网站直接把竞争对手列到一起，对于市场的优化作用可想而知。这个例子说明决定信任成本的不仅仅是互联网的结构，内容展现的形式也是有有影响的，因此设计dweb时要考虑采取类似做法。 互联网社会组织形式互联网是一个社会，那么就一定有一种组织形式。流量经济可以视为一种无政府的状态，其本身的存在不需要信任、货币。对于一个小说或者视频网站，用户付钱观看，内容被当做商品，这可以视作一种市场。分布式网络也分几种，以区块链为代表的市场，无政府的一些基础协议（如BT），然后是我推崇的信任网络。 信任网络是一种抽象的乡土社会。 开放许多小的互联网团体是存在的，不受流量经济的污染，团体中互相分享资源，形成内容生产消费之闭环。但这样的团体再多，也不能普遍的惠及互联网的每一个人，况且一个团体的人数越多，价值亦是指数级增长。若真的想达到好的效果，必然要开放，毕竟互联网的本质就是互联，而互联以高互联度为目标。 分裂中心化互联网导致内容的分裂，信息的孤立。因为据守自己的内容可以换得经济利益，禁止转载可以避免他人获得自己的流量。 思想我势必要研究清楚传统互联网的运作机制，才能决定未来的分布式互联网如何发展。 他们所开发的项目大约都是没有仔细地思想过，因此也不能很好地解决现有的问题。 比如我们建设的不是文件分享网络，而是内容生态系统。文件分享网络早就有很多了，但这样单纯的协议是没有什么用的。 单纯的协议不能和流量经济竞争。 分析流量经济时的切入点通常是经济利益，这招很有效。 关于博客本身最近加了一种final和draft的标签，draft可能是写给自己看的，或者文章结构不太好理解，final是给别人看的。 内容时常有重叠，这似乎没有什么办法，但final里应该不会有。 搜索引擎完全依靠文本匹配内容不切实际，因为最匹配的内容是指定关键字的不断重复。 用PageRank之类的算法肯定是从已有的大网站导出权重，这只会增加互联网的中心化程度，或者说是巩固。 计划下一篇文章可能写的是内容分裂、孤立之类的。 信任区块链是直接消除对信任的需要，除了对区块链本身需要信任。 The dweb是减小信任的成本，毕竟不可能完全消除对信任的需要。 开源 开源软件不一定自由，但自由软件一定开源 Apple产品代码开源，然而是彻底的不自由 捆绑销售、限制同品牌产品的做法都是违反自由软件精神的 运行自由软件一定需要自由的硬件，因此一个自由的设备必须可以刷机 Nvidia只支持某些系统，违反用户使用的自由，是开源的敌人 Apple产品只开源代码，本身根本不自由，极其虚伪 内容包括文本、视频、软件、游戏，内容是很广泛的概念 内容自由比信息自由概念更广 一旦开源软件开始限制使用，为额外功能收费，就不再是开源软件了 人的思维 人的记忆是结构化的 人建立事物的联系，概念与概念之间，事物与情感之间 人更擅长记忆结构化的信息 悖论 竞争造成信息的割裂 垄断不割裂信息，但恶化互联网环境 市场竞争是问题根源 图书馆 图书馆只是政府、私人建立的小型内容生态而已，且这种生态是只提供阅读，而不包含内容生产的 图书馆依存于全球的流量经济系统 图书馆阅读方式落后，媒体类型单调，远不及流量经济的影响力 图书馆受众小 图书馆不便捷，自由度受限 表情图的传播你肯定知道一个有趣的现象，图是越传越绿、越烂，水印越来越多。 每一次传播，质量可能下降，但不可能上升 各平台倾向于降低图片质量、压缩图片，因为节省成本 有关人员和平台倾向于添加水印，因为有经济利益 讲这个例子有什么用？类比，表情图就是信息，信息都是一样的。 假设原本有人提出一个观点，经过上述三点规则的蹂躏，最终被人听到的论点大约面目全非了。 信息源的质量是最高的，传播过程中逐层降低 流言、误解、各种谬误在这里产生 我们和信息源的距离越远，需要传播和转发的层数越多，我们的得到信息质量就越差。 其问题根源是因为信息不能自由传播，因为在流量经济体系下，信息只有被歪曲后，裁剪经济效益不高的部分，放大、夸张产生经济效益的部分，才能更好地传播。 这又是一个由一个小现象观察得出的结论 因此，互联网倾向于变得平台化，也就进一步加剧垄断。 人对互联网的需求 人所知道的（不需要互联网） 人所知道自己不知道的（可以用搜索引擎） 人不知道自己不知道的（需要互联网推送，新闻是典型例子） 对于某个事物，人所知道的和不知道不知道的肯定位于光谱的中间一处，越是知道不知道的越能用搜索引擎的方法主动搜寻，能提供的关键字也多。一个人最不知道自己不知道的事物只有靠别人主动推荐。 因此，the dweb有两种协议，search和feed。search解决主动获取信息，feed实现被动获取信息。","categories":[],"tags":[{"name":"draft","slug":"draft","permalink":"https://planetoryd.github.io/tags/draft/"},{"name":"advertising","slug":"advertising","permalink":"https://planetoryd.github.io/tags/advertising/"},{"name":"dweb","slug":"dweb","permalink":"https://planetoryd.github.io/tags/dweb/"},{"name":"china","slug":"china","permalink":"https://planetoryd.github.io/tags/china/"}]},{"title":"Archived melotte protocol specs","slug":"protocol","date":"2021-02-19T06:32:00.000Z","updated":"2021-03-18T12:35:24.433Z","comments":true,"path":"2021/02/19/protocol/","link":"","permalink":"https://planetoryd.github.io/2021/02/19/protocol/","excerpt":"","text":"Melotte Protocol ARCHIVED for reference Melotte is a decentralized application platform and framework, based on libp2p. In contrast to recent novel decentralized networks, Melotte focuses on the original purpose of P2P, to bring privacy and anonymity. An overview of the whole protocol stack (bottom-up): Libp2p More transports, e.g. kcp, tor Obfuscation protocol Block protocols Bitswap Compatibility protocols (possible): Bittorrent, Dat, ZeroNet (files only) Melotte bitswap (planned) Channel protocols Pubsub (gossipsub/floodsub) Propagtion protocols Channel propagation DHT Graph data structure Management chain Object-version content Web of trust Naming Filter Search Feed Routing Backend procedure Wasm validation script Wasm application backend Interface server Application Frontend Obfsucation protocolsPattern recognition is a popular censorship method, yet most p2p protocols have obvious protocol characteristics. Encryption protocols can’t solve this problem, because the protocol handshake (multistream) for encryption can be recognized by GFW. libp2p supports protocol negotiation. Unfortunately, it was proven to be easily detectable by DPI methods. Instead of using protocol negotiation, we use multiaddr-level protocol specification. For instance, the following protocols are supported, except libp2p defaults: TLS Websockets with obfuscated multistream handshake [TODO] Other candidates are possible and welcome Channel and block protocolsMelotte uses the following two protocols under the hood: Channel protocol, which is inefficient for long-term storage and unreliable. However, it has very low latency. The primary usecases for the realtime protocol are: Instant messaging. Video streaming. Block protocol, which is optimized for transmitting blocks; however it has high latency. Compatiblity with protocols other than IPFS is the main purpose of block protocol abstraction. We built one more layer over block protocol, encodedBlock, which supports delta-encoding and compression. The primary usecases for the encodedBlock are: Git hosting. Collaborative wiki. Video hosting. Usually we use gossip as the channel protocol, and DHT as the routing algorithm for block protocol. Channel protocol is only used to spread metadata by melotte. Then, DHT is queried with the CIDs inside received metadata. This avoids using gossip to transmit large blobs of data, because DHT is more suitable for this task. DHT routes on the basis of content. Content with more popularity have many peers serving it at the same time, which is better to be downloaded via DHT. Both IPFS blocks and dag are a part of block protocol. The abstraction of block and channel protocols are based on existing implementations. Channel is only one of the propagtion protocols. We don’t use ‘pubsub’ for channel protocol because we could have other things to be channel protocols. A ‘legal’ messaging software in some censored country can be a channel. Centralized web sites can be block protocols, which is a better alternative to a Melotte gateway. The site here is not seen as a node, because it has only block protocol, such that it is easier to deploy and maintain on the other hand. Channel protocolChannel protocol are used for following two purposes. It is not recommended to do anything else with channel protocol. Metadata. When a site is updated, i.e. a new version is published, the channel protocols are used to send new version CID to site seeders. Realtime data. This includes chat messages, etc. Each ChannelData MUST be signed with a publickey. 12345678910111213141516syntax = &quot;proto3&quot;;message ChannelData &#123; bytes sender = 1; // Publickey bytes signature = 2; bytes payload = 3; // Can be another channel data uint32 timestamp = 4; bool repropagate = 5; uint32 encryptionAlg = 6;&#125;// Only CID. As the payload field of channeldatamessage PropagationPayload &#123; repeated bytes metadata = 1; bytes packed = 2;&#125; In some difficult circumstances, where the connectivity of the global network is limited, we may use channel protocol to resolve CIDs and even as a block protocol, but only in small scale. Gossip protocol is more resistant as it doesn’t reveal the addresses of other peers, which makes it impossible to do so-called ‘collect IPs and ban’. Propagation protocolDue to the nature of block protocol that it is optimized for transfering blocks, which is usually not capable of ‘propagating’, we proposed propagation protocol that turns static sites into dynamic sites. There’s also another type of site called mutable site, a term from IPFS. In the context of IPFS, they use IPNS to run mutable sites. IPNS mainly uses DHT to ‘propagate’ updates, by associating a peerId with a CID that points to the latest version of the site. That doesn’t actually propagate, and it is always single-usered. The difference between dynamic sites and mutable sites is that dynamic sites allow more than one authors on a single site. For a mutable site, DHT is apparently the optimal propagation protocol, since there is only one author. Another issue is how the peers of the site can be notified when an update is released. DHT doesn’t flood the network. Not involving other protocols, the simplest approach is to set a TTL, and let the peer to re-query DHT periodically. Dynamic sites require a broadcasting protocol, such as pubsub. That’s ‘N to N’, if we treat mutable sites as ‘1 to N’. The latter is solved because DHT can always map a key to one value. The whole propagation process consists of two steps, visiting a new site, and receiving the updates. When visiting a new site, it downloads the management chain first which is technically a mutable site at this moment. After that, it collects user data, which is truly a dynamic site now. Therefore step one can be optimized using the IPNS way, while step two can’t. A basic way to solve dynamic propagation is to use a request-response protocol on channels. Each peer stores a set of values(metadata) for key K, which is normally a site address, and listens on the topic K. When a request is sent, all peers respond with the set of values that have not been sent on the channel. The requester collects as many values as possible. An obvious flaw is that metadata is also broadcasted to peers who already have it. So we don’t send metadata on the channel direcly. Instead, the CIDs of metadata are sent. To avoid the use of DHT, the sender peers are supplied to block protocols, so that the metadata are immediately fetched after getting the responses. Note that DHT can also be used here, especially for packed payload (see section site). The CID here has two purposes, to let inform the requester the new data, and to remind other peers what have been sent. Suppose we pack all CIDs of metadata into a single block, which is totally feasible, but other replier peers have to download the block, or they won’t know what have been sent, and might send repeated metadata. Optimization: Packed payloadThe purpose of packing metadatas is to take advantage of structured network, namely DHT, for some data that isn’t often changed. DHT isn’t suitable for publishing updates, as it only notifies some closest peers to update some record. Assuming there are CIDs of metadata, M1, M2 and M3, sender A packs all into payload P1(M1,M2,M3), which is also a CID. Sender B could download the content of CID P1(M1,M2,M3), or he can look up in the cached entries if there is. Assuming M1 is a very old metadata, most peers naturally won’t include it into the packed payload. When a new metadata M4 is released, the packed payload has to be updated. Let’s call that cached/packed payload, and payload caching. The basic rules are that new packed payload are generated by the network periodically. Old metadata are excluded from packed payload. New metadata are included after a reasonable time, when it is fully broadcasted and accepted. This protocol looks like a sliding window: 1M1 M2 [M3 M4 M5] M6 The packed part is the so-called ‘not often changed data’, and the data after that is the items waiting to be packed, after a certain time period when it is considered optimal.Packed payload is served for peers visiting it as a new site. Both packed payload and non-packed CIDs (M6) of metadata are sent over channels. As for possible attacks, a peer could respond with some outdated version of a site. If it isn’t the only peer, subsequently any other peers would act as whistle-blowers, spreading the latest version of the site. Optimization: DHT As shown in the graph, a site usually has only a subgraph of nodes, while all nodes in IPFS support DHT. Obviously, the more nodes, the more censorship-resistance. For all data, including management chain and user data, it’s possible to use DHT for update resolution, not publishing. Management chain is resolved recursively by all known pulickeys. An object might have multiple intended branches. If there are multiple users working on that object, the pubkey should resolve to a branch that hasn’t been mapped to any pubkey. An abitrary branch is chosen if one user has multiple branches. The shortcoming of DHT is that it can only map a publickey to one CID (Actually PeerId in IPFS’s implementation), and it can’t check whether the mapping is valid according the site defined rules. Hence channel update resolution will be performed soon after that. A propagation field is added in management chain, so the network works even without channel protocol. However, it is unsafe to use a site without channel protocol, because the site is incomplete. For example, the site owner has withdrown the permission of a pubkey in the chain. Due to the single-mapping characteristics of DHT, when the owner is not known before the block that adds the new pubkey and all other owners are already withdrown, the withdrawal branch is invisible by performing only DHT update resolution. So, the pubkeys specified in the genesis block can’t be all replaced with other keys. Otherwise the site might be hijacked during initial DHT resolution. This optimzation would work well for small sites, often single-usered, with only a few peers. IPNS validates records only when resolving, but not when putting DHT value. This means it is possible to override a valid record with random bytes for an attacker, which slows down the resolution of a site. In conclusion, it doesn’t make sense to use existing IPFS DHT for site resolution, for it causes many security problems and limits the features we could use. We need a specialized DHT for melotte in the future, which adds conditions to updating records, since structured network is usually better. To solve the withdrawal issue of a publickey, we introduce a withdraw flag in the record to announce that it shouldn’t be updated anymore. This is an extra validation process of DHT itself, which is performed before any store attempt. We also need time guarantee, preventing further updates to a withdrawn record. During the process of looking up a key, any peer reporting any withdrawn flag terminates the entire process. Related sybil attack can be solved by peer reputation, a subset of WoT. EncodedBlockEncodedBlocks are used to transfer objects, which is a collective term for all raw data. The reference that a block uses for delta-codec is called base EncodedBlocks contain object content in a compressed and encoded format. Encodedblock, also known as, block. For instance, the following formats could be or are supported: Delta-encoding: a block is compared with its historical blocks and other related blocks, and instead of raw block content, two values are stored: A list of bases, i.e. references to some older blocks Delta, one of the codecs is: Copy action allows copying a part of some base to the current block Insert action allows adding arbitrary new data For instance, if the base contents are ABCDEFGHIJ and QRSTUVWXYZ, then the whole English alphabet can be efficiently represented as: Copy bytes 0..10 from the first base Insert letters KLMNOP Copy bytes 0..10 from the second base Compression: object data is gzip-compressed Additionally, the formats can be stacked on top of each other. Notice that order matters: i.e. first delta-encoding and then compressing data is more efficient than first compressing it and then delta-encoding. The first way is known as stacking compression on top of delta-encoding, the second way is stacking delta-encoding on top of compression. In dynamic block mode, the CID that is announced on the DHT is a hash of the actual non-encoded data, ie. versions, not the hash of a datablock. EncodedBlocks can be sent to other peers in the following two ways: Per-connection, or dynamic block mode. This allows using a different codec or different data for each peer. This method may be slow but it is way better for network connectivity: a peer which doesn’t support a new codec may still receive new content, while others peers which support it will receive it compressed even better. This method, however, requires developing a new protocol. Compatiblity, or Static block mode. In this mode, the same datablock is used for all peers who request a specific object. The CID announced is of the datablock, rather than its actual data. This allows using the Bitswap protocol to transfer EncodedBlocks. However, static block protocol is inefficient in some cases. When the compression methods are updated, the majority serve the content using new codec, so the blocks of old codec are rarely served, splitting the network, which decreases overall performance. Besides compatibility and consistency, static method is rigid, for it can’t encode the data dynamically based on the circumstances of the receiver. A datablock has a slightly different meaning for these two modes. In static block mode, a datablock is effectively a valid IPFS block on its own. In dynamic block mode, a datablock is temporary and abstract, and it doesn’t have to be stored directly in the filesystem; however this caching may still be done for optimization. In both cases, due to the nature of a datablock, it is perfectly valid to make a new block and claim it is a newly encoded version of an object. Although the object content is signed by the content author and thus can be easily verified, a datablock cannot be verified until it’s decoded. This gives opportunities for DoS and allows exponential data size growth, even when the datablock size is small: for instance, each block could duplicate to the previous one by performing the copy action on it twice. A simple hotfix for this problem is that the object signer should also sign the datablock; however, this fix breaks when either of the two conditions hold: A block author is not trusted. This is a common situation in multiuser sites. A block author may leave the network. This hotfix effectively rejects any updates to a datablock after the author leaves the network. Another solution is proposed instead. It is well-known that most if not all modern compression algorithms, and thus codecs, allow getting unpacked data length quickly. This allows checking if data length is not greater than the maximum allowed length before actually unpacking content. 12345678910111213141516171819202122232425262728293031323334353637383940syntax = &quot;proto3&quot;;message EncodedBlock &#123; message codec &#123; DeltaType delta = 1; CompressionType compression = 2; &#125; message Action &#123; oneof action &#123; CopyAction copy = 1; InsertAction insert = 2; &#125; &#125; map&lt;uint32, bytes&gt; cids = 1; repeated Action actions = 2; repeated Link links = 3; bool encrypted = 4; uint32 encryptionAlg = 5;&#125;message Link &#123; uint32 cid = 1; // CID, not multihash string name = 2; uint64 size = 3; // Tree size&#125;enum DeltaType &#123; plaintext = 0; binary = 1;&#125;message CopyAction &#123; uint32 base = 1; uint32 offset = 2; uint32 length = 3;&#125;message InsertAction &#123; bytes data = 1; &#125;enum CompressionType &#123; zlib = 0; &#125; Delta-encodingWhat can be bases ? All types of versions, whether the version is a raw block, or delta-encoded. Objects can’t be bases. However, referencing objects from another site might be useful. Typically, the bases selected to encode a new version of an object are the older versions, unless the new version is considerably different from all its parents. So, can we use bases outside the site ? How they select the bases for a datablock can potentially cause undesired seeding. Once we visit a site, we trust the site and seed its content implicitly. The problem is that a site can have many users. You may want to seed only the content of the owner. Imagine a blog author posted an article containing a clip of a video that is 10 GB. Assuming there is an insane 10 GB block, which is not sharded, you have to download the whole block before processing that clip. In reality, the video is splitted into tens of thousands of blocks. It is not impossible to have a 10GB raw block, however. In such case, we trust the site author, so we actually choose to download the 10GB video. If a random blog commenter uses su.ch a clip with 10GB base block, we probably won’t do the same. Firstly, we need a system to manage what can be implicitly seeded, and what can be used as bases in what scopes for what kind of users. Then, it should ask user if existing rules can’t decide. How to encode with optimal performance ? There are multiple factors for choosing bases. How much extra data we need to encode using some bases, whether the bases are seeded by the majority, and whether the bases are seeded by the majority of the visitors of a site. For example, d-stackoverflow users usually have d-stackexchange seeded, so we can use resources from one another. When encoding a datablock, melotte search from its the parents of the version, and from the scope of same site. It doesn’t download new sites automatically. Non-downloaded blocks can’t be compared and used. Encoding from another site requires the help from the user. Application assisted delta-encoding is preferred. A site can specify bases when a user comments with quotes from other comments. Melotte would offer an interface for users to choose bases, and the scopes to search bases. The size of a blog comment is usually size-limited. To get more ‘real’ comment space, users are encouraged to use good bases. Quoting or linking is also a good practice. This is the natural ranking system. Most quoted content gets more seeders, and quotes can be traced (what site a comment belongs to) if it is not quoting low-level blocks. Quotes are authenticated, moreover. How should size-limit be done ? Two different limits, one for actual datablock size, another for decoded data size. Factors concerned Encoding cost, time of searching, and space of downloading new blocks if melotte is configured to download new blocks for encoding, which is disabled by default. Decoding cost, time of querying DHT and space of fetching extra data that is not included in this datablock First datablock cost, when the datablock is the first block a peer wants to download First datablock of the site cost, when it is the first block downloaded in site content. First block of a scope, the same as above, but for a scope, which can include multiple sites. Extra cost, when there is a existing alternative base on that peer, but per-connection delta-encoding is not available. A datablock might include many layers of CID reference to other EncodedBlocks of current site. When this block is downloaded, the whole site is almost completely downloaded too. Though you can get other blocks quickly after that, the time to download this block as the first block is significant. The cost of querying DHT is mainly about the time. Actually the downloading would be fast if there are enough peers. The time of querying is much more different. It involves network latency, and varies depending on the network circumstances. Extra data is the data not really required by current datablock, but can be useful later for other blocks in the same site, scope, or for that user. CompressionCompression can also be done on transport layer, which needs re-computing every connection while the data stored is not compressed. The compression on this layer gives the user to save the space of storage, for some specific objects or sites. Since it is per-object compression, only some of the blocks are compressed, allowing frequently used blocks to be not compressed. Per-connection delta-encodingInstead of encoding objects at publish time, this can encode for each receiver peer. The above requires the content author to choose bases for his audience, which favors the majority, and might be inefficient in some cases. This mode allows to change the datablock and the encoding for an object as time passes. It doesn’t mean it encodes for each peer, but it can. Through a delta-codec negotiation process, the sender finds out what blocks the peer already have, and the bases are chosen based on that peer. This mode is not compatible with bitswap, as it uses the actual CID of the payload, ie. decoded data, as the announced key. VersionedBlock12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849message VersionedBlock &#123; uint32 timestamp = 1; map&lt;uint32, bytes&gt; cids = 6; // Reused cids message Parent &#123; uint32 cid = 1; BlockType blockType = 2; uint32 size = 3; string type = 4; &#125; repeated Parent parents = 2; // Previous versions of this object message Reference &#123; uint32 cid = 1; // One of the versions of the target object string branchName = 2; // As a new branch of this object string referenceBranch = 3; // master branch by default &#125; repeated Reference refs = 3; oneof data &#123; // Data of this version uint32 cid = 4; // Cid of the encodedBlock, compatible with bitswap bytes block = 5; // Can&#x27;t be archived if using this field &#125; uint32 cidData = 13; // Cid of the actual data (which will be used in the // future as the default DHT key). Also for archiving bool explicitBranch = 7; // Explicitly mark this version as a new branch string branchName = 8; // Optional string tagName = 9; // To mark important versions uint32 verifierId = 10; // ID of the verifier for this block in mgmt chain // This can be viewed as important associations, which will be downloaded by // melotte // Non-optional deps will be downloaded automatically for optimization message Dependency &#123; uint32 cid = 1; // CID of a block bool optional = 2; string name = 3; // Application-specific string type = 4; &#125; // Association is a preview of all the links this block has message Association &#123; uint32 cid = 1; string type = 2; // Arbitrary type defined by application &#125; repeated Dependency dependencies = 11; repeated Association associations = 12;&#125;enum BlockType &#123; raw = 0; // Any blocks from IPLD versioned = 1; // VersionedBlock&#125; VersionedBlock is mostly what a version of an object is. The versionedBlock architecture is a git-like version control system optimized for the decentralized web. The notable difference is that it tracks objects individually, rather than treating the repository as a whole. Besides regular links in IPLD DAG, we introduced some special links, parents, dependencies, and associations. Those are common things a site would probably use. Parents stand for the older versions. Dependencies describe the logical hierarchy designated by the site. And finally, association is the what that block links to. This is called semantic linking, which makes it possible to analyze what links and what type of links a site has without actually downloading the content. Association is different from Base. A base is about data, while an association is about content. Any block can be a version, so a static site from IPFS can be transformed to a melotte site. On a higher level, Reference is used to mirror objects from another site, like git submodules (see next section). There are two types of branch in the terms of melotte, explicit branch and actual branch. Obviously, actual branch is literally what you see, which is similar the ‘branch’ in blockchain. Explicit branch is the ‘branch’ in version control systems, which is marked by explicitBranch here. If we use git structure directly on IPFS, there will be hundreds of unintended actual branches and conflicts for a large project where many people work at the same time. The decentralized web doesn’t guarantee every pull one gets the latest version of that repository. Melotte splits the repository or a site into smaller units, so a confilct on a file doesn’t change every hash of every node on that merkle tree. In fact, CIDs of all those objects are still assembled into a metadata for propagtion. The difference is that ipfs-git has to propagate the root hash of a repository, which has unnecessary overhead. Melotte can propagte one single object of a site, because the metadata is ‘flattened’. In other words, the separation of update propagation and content structure. Normally, the folder of a file isn’t its dependency. The folder isn’t needed to be downloaded to make that file meaningful. It’s a structure, about how filesystem organizes, so we use the links within EncodedBlock for such purpose. SiteA site is defined by a set of rules expressed in WASM, which is evaluated on each node of this site. All of the nodes of the site have some or all of the objects. The rules can refer objects from other sites to expand the objects the site have, and specify some administrative publickeys to manage the site and modify the rules. The nodes having this site form the network of the site, but the site can actually have objects outside its own network. The bottommost layer is libp2p, where the DHT is shared. On the higher layer, Melotte defines some basic protocols all sites share, eg. channel, search, and feed. Applications serve for particular purposes, so the rules and data differ. The concept of site is adopted since sites usually don’t overlap. The overlapping areas of sites are optimized with cross-site operations, to share objects and code. A site consists of a management chain, and its data. Site data is a collection of objects from any users permitted by the management chain. An object is the basic unit a site operate on, which is made up of one or more versions. An object is denoted by one of its versions, of any branch belonging to this object, that has a same initial version. Metadata is used to propagte new versions of site objects, mostly in a ‘flattened’ manner. Objects are connected in the form of parent, dependency or association. Such connection is different from IPLD, since objects are mutable. Publishing new versions of a site or its user content can only take place on channels, because you can’t put a link pointing to the new version in the block of the previous version. Given a genesis management block of a site, it’s impossible to get its sucessors without channel protocol. Another aspect is user/site content, which is basically aggregating based on some rules, ie. data script. In addition to the graph, when the author signs and publishes the metadata of his blocks, other peers listening on the channel cache the received metadata in the repo, as if they are blocks, and re-propagate the metadata when other peers request it. For each version of an object, the signer generates a new metadata. The dafult behaviour (well-behaving) of a peer is to re-propagate the newest version of an object, since new blocks normally link to old blocks. To identify which block you got is the successor of an arbitrary block, the successor block (or its metadata) should contain a link to its previous version. Each block of an object is called a commit or a version, which can have bases if it is a EncodedBlock. Commit is the action of publishing a new version A user publishes a request on channel, and other peers response with all related data. This doesn’t guarantee the upreventn one request window, there can be one request, and metadata of site and user data. An improvement is to put metadata on DHT using the publickey of the site as the key and the site metadata as the value. More precisely, the publickey of the genesis block and when the management chain is downloaded, the publickeys of all other known signers. Each publickey maps to the latest metadata signed by it. You still can’t use DHT to replace channel protocol, because there’s no way to notify peers that something has updated. So, IPNS uses a polling method. This way is only useful at the first time visiting a site. After the first visit, we’ll use channel instead. The hierarchy looks like this Site Object Versions Bases Blocks Branches are separate versions Don’t confuse melotte block/object and ipfs block/object. You can also treat each folder as an object. That depends on your need. The benefit of tracking files individually is that you can have different permission settings for each file. If the whole folder is treated as an object, any modification to any file inside produces a new version. In this case, to set different permissions, the site has to do version control manually. Site accelerates routing. For any peer of a site, it is likely to have most site objects, especially the objects marked as non-optional. For an object, the peers serving it are also likely to have its dependents, dependencies and other related objects. In these circumstances, the query of DHT only needs once. How sites are created, and organized with other sites: Template. A site can be created from another site, called base site.Base site can contain a constructor fuction to create the new site. The new site can have all objects the base site have, and the objects use the objects from the base site as bases. A site can also be created without constructor, by selecting data manually. Reference. Objects from a site can be linked to another site. The objects linked to that site update when the source update, which is different from Template. Component. One site can call another site via melotte. That site, as the callee, is called component. A name provider is a component site, which is completely independent, but requires other sites to be fully functioning. Normally, a site has several parts of data Management chain Management script, usually compiled WASM binary, inside the management chain, for verifying the chain itself and the data. Data script, also WASM binary, for validating the site content, including both owner content and user content. Data Backend, where site data is read and written, and it interacts with frontend through melotte. Frontend, pages served by melotte interface server. Site content, also known as site data. Case one, you want to add a commenting section to your blog. A real world example is disqus, but you probably don’t want to use that. The common way is to add some code on one’s own, or use a package manager like npm, to install a package. That acutally works, but you have to update the package when a bug is fixed or new features are released. A complete commenting system has management interface, that requires a backend. Integrating so much things into a blog isn’t a good idea, so, we introduced component. If you don’t want to obfuscate management chain and data script, a component shouldn’t be allowed to write into the site. Instead, the commenting component should have a feature to download only a part of the comment data, which is only related to your blog. The request of downloading is, of course, all sent by the data script or backend. Melotte doesn’t download anything except for the chain and the data of owners specified in management chain. Case two, creating a new d-stackexchange. There are many ways to do this, copying the code from base d-stackexchange and pasting to a new site, or referencing the code of base site in the new site. Before creating a new site, we should check our requirements. Will we do changes to the code base, or we only need an identical copy of the original site. Generally, we’d solve that by using references, which are like git submodules, but operate on the level of objects. Suppose we don’t do any modification, the references get updates automatically without owner’s confirmation as there are no conflict. The problem is we always need to do something to the code base, which causes conflicts. When we commit the changes, we actually have two branches, one is our commit, another is the object from referenced site. We have to merge these branches, or use one of them, through some process. Management ChainManagement chain is the core of a site. It determines what data is allowed and not allowed for that regarding a specific user, and what new blocks are accepted for the chain itself, etc. Each succeeding block is verified by its predecessor. To turn a single-owner site into a multi-owner site: This is how multi-owner site is done in melotte. Owners in a site are equal. Another way is to use the CID of the genesis block, containing a list of granted publickeys, as the address of a site. That way works if we’re creating a new site. The DHT based site propagation protocol queries publickeys defined in management chain recursively: firstly pubkey A, then B and C, in this example. Accidental branches are usually tolerated, depending on the rules of verification script. Since every publickey has only one ‘pointer’ to the latest block it acknowledges, branches are not guaranteed to be known by peers. A block is allowed to have multiple parents, to avoid losing branches. Now management chain is more like a single special object. Some common site management methods: Single-owner Most censorship-resistant, because this kind of site can be resolved without channel. Multi-owner, with no limitation Voting based multi-owner Blockchain based multi-owner ObjectsAn object is a series of versions which are VersionedBlocks. It is meaningful only when its site is present. Objects need a site as the topic name of the channel to receive further versions, so there shouldn’t be isolated objects. Object can have references, which mirrors some branch of the target object as a single branch. This is not git reference. Reference is marked by some fields in a version of an object, including thewhy mining pool gets bigger address of the target site, and the CID of one of the versions of the object. The virtual branch is created using that version as the common parent. Git-like branching system can be implemented via data script, which decides the current value of an object. Reference is always about a specific branch of an object, as there could be other branches parallel to the referenced branch, having common ancestors. Version-object structure is similar to git, but not exactly. We aim to offer versioning feature while keeping maximum flexibility. A default data script is provided to deal with branching issue. There is no concept of merging in a decentralized network. All branches are kept. A site can either keep both branches or select one of them, or anything. That’s the responsibility of the site, not us. All versions are kept for data script. One can still commit a version that uses several branches as parents, creating a new branch that merges two branches. Accepting this attempt of merge is also determined by the script. Branches are immediately created when someone commits a version with more than one parents, or more than one versions use a same parent, which is different from git. Anyone accepted by data script is allowed to commit on an object. If two persons commit to an object at the same time accidentally, there will be two branches. For a wiki site, we may show the version with latest timestamp to users. In this example, order doesn’t matter, but the latest version is preferred, because the latest version means it is more ‘accurate’. Suppose we don’t use the latest version for a wiki, a false wiki page can’t be corrected immediately. For the code base of a site, we can’t simply use both branches, which might break the site. Based on time, we can choose one of the branches with earliest timestamp. If someone commits to that object and use two branches as parents later, we can simply switch to this branch. Such a branch is called a merge, which has multiple parents. A merge that use all existing branches, for a specific peer, as parents is called a complete merge. Conversely, a merge that only solves some of the conflicts is called a partial merge. In this case, we may only want a complete merge, and choose branches or merges with earliest timestamp if there isn’t. Using the earliest branch is to keep the code base stable. We can’t use longest branch here, but time gurantee works. It is possible the privatekey of one of the signers is stolen, and the attacker attempts to create a branch on a very early version. For a blockchain site, use the longest branch then. Timestamp is assumed to be accurate, see section time guarantee. Archiving and pruningIn dweb, archiving is a process to formally announce some of the data will no longer be seeded by the majority. An Block or Object has two states, by design. (state isn’t and can’t be a field in the block or object. It’s the result of observation). This is yet another difference to git, as we have to care about the efficacy of seeding. We avoid keeping both archived and non-archived form of a same object at the same time, which splits peers into two groups if they don’t seed both versions, greatly reducing censorship-resistance. Alive, the data still being propagated on channel all the time. Archived, the data is no longer a must to be downloaded This concept is proposed for it’s a common requirement in sites. When we say archive a block, the block may be a raw block or an EncodedBlock. The meaning of archive can be archiving a version of an object, or a base of a version, or the entire object inluding all its versions. The method of archiving also varies. We can archive the bases or the blocks or objects that depend on the bases, ie. dependencies. It’s ambiguous, on what is being archived A random example of how complicated the relationship among blocks can be One of the concerns is when you archive something, there can be side effects. For instance, assuming you are going to archive a base, to make the base itself no longer a requirement of any other blocks, you have to traverse through the merkle forest, because a base doesn’t have links to its dependencies. You may propose to create reverse links when receiving data, but the choice of archiving bases is wrong in the first place. Now think about a base can be anything that other things depend on, not only a concept only of delta-encoding. So, we don’t archive the dependents, but the dependencies of some data. Note that there are always one or more versions regarding an object, whether it is delta-encoded or not. There are numerous forms of archiving: Archive the historical versions. For VER3, it is VER2 and VER1. Archive an object, OBJ B Archive a site, Site 1 When archiving preceeding versions of a version in an object, the payload of the EncodedBlock is simply replaced with the decoded data, whose CID is already written by the signer in the EncodedBlock. That actually removes all the bases, thus not depending on its older versions, and any other blocks. This might be inefficient when delta-encoding is efficient, without any previous versions as bases. A solution is to use dynamic delta-encoding described above. Objects can also be archived, and is straightforward. Exclude the undesired objects from site metadata, since an site metadata always include all its objects. The archived objects are automatically cleared via garbage collection, when it reaches the size limit. Explicit archiving can be instructed by a version that has an archive field. An extra process is needed to archive a site, that the owner needs to publish an Archive Metadata to let peers unseed that site. MetadataMetadata is the connection between blocks and channels, which includes the basic information of the signer and some CIDs of the site. It tracks the current version of every object. The question is how much data is necessary to put into metadata, but not a reference in metadata. Block protocol is optimized for blocks, although DHT query might slow at first. Channel has lower latency, for small packets. It is considered faster, when the cost of DHT is more significant than the benefit of de-duplication. If we use block protocol as much as possible, replacing metadata with the CID of it, we have to query DHT for metadata before everything. Obviously, we can’t, as explained below. Metadata must contain authentication, identity and timestamp information. Another extreme case is to put everything into channel. That makes sense only if the data is single-use and is hard to do delta-encoding, such as random bytes. The previous metadata field isn’t needed, in fact. A single metadata contains all the metadata information for its corresponding site. It is never delta-encoded. That field is used only for those who is willing to seed archived objects. What if there is a giant site with one million objects ? Suppose we use SHA-256, and suppose it is 31250 KiB, about 30 MiB every metadata. According to ipfs-unixfs, the minimum reasonable block size is 0.25MiB, which is about 8192 hashes. Any metadata with the number of hashes above it is inefficient, in general. For such a giant site, we use field subMetadata, and each layer can contain up to 8192 block or subMetadata CIDs. In this example, it uses two layers, which has 8192 metadata blocks in the first layer, and each metadata block has 122 CIDs of objects. The download process is handled by data script, which decides when the metadata is received, what blocks to download, when to download, and the depth, the priority and so on. Extradata can store information for such conditional download. “Optional objects” is not managed by the network, but the script. Because ‘optional’ is ambiguous. Does ‘optional’ mean to download when needed ? However, what time it is needed, when requeting, or before requesting to improve user experience ? Those dirty tasks are handled by the data script. Of course, we provide a default script. Metadata is always about a user, whether it belongs to site content or user content. There’s no strict separation of site content and user content. The data script determines what is considered site content. Blocks from each author of the site form objects of the site. An object can have versions from multiple authors. Only top-level objects or blocks are sent via metadata. Top-level blocks or objects are not referenced by other blocks or objects immutably, which are not accessible without metadata for the data script that downloads the site. Metadata script would inspect metadata each propagation, to filter out malicious unnecessary data like non-top-level CIDs, and archived blocks or objects that are not needed any more. Denial of serviceThe cost of being DoSed depends on the protocol. In the worst case for the channel protocol, attackers generate a new public key for every message. Melotte has different threshold for Metadata and RealtimeData, because the former is metadata only and is always size-limited. Metadata is processed as follows: Decode protobuf. Filter off banned peers. Verify public key and validate message. * Keep this data for later processing. * Forward it to other peers. Only step three and four are vulnerable. However, with peer reputation system and conditional forwarding, channel spam is efficiently suppressed. When we have a CID of a DataBlock which we received from a trusted peer, we do the following: Download datablock via the block protocol. * Decode protobuf. Filter off this block if it’s signed by a banned peer. Unpack block, e.g. with delta-encoding. This step may require downloading other blocks. * Verify the hash of unpacked content. Proceed handling the data on upper layers, e.g. running validation scripts. Web of trustThe idea of web of trust is that, the only trustworhy person is yourself. Web of trust is subjective, which forms a different directed graph for each peer. Let the count peers you directly trust be N1, and the count of peers trusted by each peer you trust be respectively M[1], M[2], … , M[N1]. So, in the third layer, the trustworhiness of the first peer is 1/N1/M[1]. Peers trusting each other on the same layer are counted. A peer on a layer trusts several other peers. They get the same trustworhiness if they don’t trust each other, or they trust all each other. The peers trusted by more peers on the same layer get a higher share of trustworhiness arranged for that layer. Peers on nearer layers can trust the peers on further layers, from that peer’s perspective, which means the reverse trust is not counted. Such a cross-layer trust is actually about how we treat layers. In fact, one peer can be on multiple layers, and obviously the trustworthiness is calculated individually and added up afterwards. Announcing a trust record is similar to normal site content, but the validation is handled by Melotte. Trust records are resolved and downloaded recursively until the evaluated trust values of the edge peers became insiginicant. One of the purposes of WoT is to encourage data sharing, which also has been accomplished with other technologies such as FileCoin, where contributions are recorded on a global ledger. As said above, we don’t use a blockchain, but things are similar on a WoT. The more people know your contribution, the more encouraged it is to share. Therefore, we have a balance between the amount of WoT to download, and how encouraged peers are to share files. The issue of seeding and sharing is naturally solved, since WoTs are formed within peers with common interests. 123interface TrustRecord &#123; // Encoded data on datablock trusted: &#123;key: Pubkey, transport: number, application: number&#125;[]; // The trust values in different aspects&#125; There are only two options, either blockchain or WoT, to prevent sybil attack and offer a reasonable functionality.Blockchain is not censorship-resistant, however. WoT is not a site, but a part of the core, although it is basically a site. There is a UI for managing WoT, which resides in the site that distributes WoT records. The records of someone are not downloaded until being trusted or distrusted. Applications may trust other users automatically based on the behavior of the user, simplifying the usage of WoT. All applications share the WoT data, along with lower protocols, which creates the possibility of abusing WoT. To address this, a comprehensive permission mangement system against applications is needed. There’re two types of trust Weak trust, indirect interaction, from the statistics of transport protocols, and other automatically derived parameters. Direct interaction but automatic, eg. completing a captcha from a user. Strong trust, direct interaction, when the user manually trusts someone. We mainly use weak trust, since it is rare to get strong trust data. Following every person you see is boring and unnecessary, which is also why most WoT based networks fail in the past. The other type of weak trust is to the peers you don’t transfer files directly but indirectly met on a same site. For instance, when you browse through a social network, a forum, if you don’t block some user, the user is implicitly weak-trusted. This means WoT is best to be verbose, that is to say, a peer should reveal all its known information. A peer may either trust, block, or be neutral to another peer. Weak trust is better than neutral as long as there’s any information. Implicit trusting all non-explicitly-blocked peers might mistakenly include spammers, which doesn’t matter as we have a web of trust, that any strong trust blocking someone in the network overrides the weak trust. This makes joining a WoT much easier. Send some reasonable messages on a site, and you get some weak trust. A spammer is immediately banned once he starts to advertise, because anyone in the WoT can override the weak trust. Weak trust can be distributed with more conditions, the activity of a user, the frequency of posting, a blocklist of all undesired words of the user, and even an AI to detect bots. Weak trust is the result of computation of an individual peer, which forms a network of spam filtering system. A site may also set a higher entering requirement of trust value, making weak trust insignificant for this site. DHT distributes the records of blocks equally among all peers, including those untrusted peers. In WoT, the peers trusting some other peer are more likely to have his objects. Trusting means the peer has at least interacted with another peer, such as downloading that peer’s user data on a site. To any peer in the WoT, it should have some objects of the nodes directly and indirectly trusted. The probability incrases for nearer peers, so we traverse the graph from the closest peer. WoT routing is identity-based, and DHT is content-based, but WoT routing only works within the subjective WoT, which doesn’t scale. The blue circle represents the nodes that trust or being trusted by the node, which is the limited field of view of a single node in the network, since a peer can only store a limited amount of WoT records. WoT deals with the trust between peers, but not the trust to the ‘content’. A peer trusted by you may comment approvingly to a post, which implicitly indicates the post is not spam. He coul also announce on WoT, which incrases the amount of records all his followers need to download. So, the trust to some content doesn’t relate to WoT, or add any burden of extra records, which belongs to the content, managed by the site. This kind of trust is ‘trusting with WoT’. It also has drawbacks that the trust doesn’t influence other content. Implicit trust This is the third type of trust. For it is strongly connected with the content, that requires the calculation by the site, implicit trust is only used in limited connectivity. WoT based spam defense are applied on higher layers, block protocol and site content. When exchanging blocks, peers with higher trustworhiness are prioritized, which is increased by user or automatically set according to the behaviour of the peer. Normally, user specified trust records have much higher weight. On application layer, the content shown to the user is based on WoT and site itself. Since the user implicitly trust the site when visiting it, the site has the right to ajust the ratio and influence of WoT evaluation result. The actual problem is about protocols, where you can’t rely on human decision. WoT prioritizes known and trusted peers, and there might be DoS attack consuming limited bandwidth remaining for new peers. In DHT, we may place the most trusted peer in the buckets, instead of the most stable ones. Then, when we query the DHT, and find a known trusted peer in the responses, we tend to get more known and trusted peers once we start querying from that one peer. One possible solution is to ask the requester peer for a captcha, or even user-specified challenge. If the peer passes the challenge, he gets the trust, less or more, from the challenger. In other words, he joined the Web of Trust, since the requester is also trusted by others. Depending on the difficulty of captcha and the circumstances of the challenger, or WoT, he may need to do one or multiple captchas from one or more users. Traditional F2F protocols are limited to a few peers, as new peers are not welcomed and easily joined. All of such protocols failed, in terms of popularity. The network would be broken or splitted if everyone distrusts some other one who has some different opinion. Even a spammer is a necessary part of the network, for we have one purpose in common, running this network. People with different political views may block each other from their sight, but they can and should keep the connections of the protocol at the same time. So, a peer is only truly banned when it is both weak and strong distrusted. In a larger network, unlike those F2F ones, the peers you connect seldom overlap the peers you see on a site, which means only weak trust is usually available for direct connections. Weak trust and strong trust are also distinguished in a trust record. Purposes and reasons of trust Disagreement and agreement on opinions Personal preference … Spam judgement Transport protocol We trust differently for different purposes here, which is not followed by those F2F networks in the past. For example, the distrust against someone by some other one’s preference shouldn’t be interpreted that the person’s content is spam. On the bottommost layer, we have the largest WoT, which consists of all peers willing to share data. On the higher layer, we get the WoT for applications. The people interested in some topic are likely to trust other people having the same preference. The peers willing to share files only trust other such peers. The trust generated from agreement or transport should be treated separatedly. If these two are not distinguished, it will be harder to get the right WoT for either application or transport. The peers trusted by the tranpsort protocol don’t necessarily share a common interest with you, which lowers the efficacy of WoT. Adding the two trust values together is not feasible either. The set of peers with transport trust are not a subset or superset of the application trust. Similarly, we also separate the trust for some peer not being spammer (weak trust) and the trust for someone with a common interest (strong trust). However, the relation between the two sets is subset, so we don’t really use two fields for this. Naming Although the paragraphs below mainly talk about DNS, the solutions also apply to user name. Existing naming solutions in decentralized networks include NameCoin, ENS. Like conventional centralized DNS, you have to buy names, but many names have already been kept by investors, who make money from nothing. Let’s consider what the purpose of DNS is. It is definitely not about investing, and one shouldn’t own large numbers of names. Obviously, DNS provides a convenience service, a mapping betwenn domain names and addresses for its users. Under the WoT, we have a solution, which gives the freedom back to users. Goals: New records shouldn’t override old records that are names connected with influential sites Old sites’ names may be replaced and associated with a new name if most users are willing Spamming records should be blockand and not intervene new sites. When a site isn’t well-known, it is unlikely for someone to register the name of that site. The records will have been distributed after the site is widely used, and so it’s far harder to hijack the name now. WoT based DNS. Each user can publish name records for their sites via channel protocol. Visitors resolve domain names based on WoT evaluation result, as follows A user can choose one candidate from all competing sites regarding a domain name, and publish a name preference record When resolving a name, the name evaluated by WoT with highest score are selected. Unfortuately, we can’t trust users completely. They might choose a phishing website for a domain name, or trust some dishonest people. However, trusting always exist. When you use a DApp registered on ENS, you implicitly trust ENS, which is an authority, although it is seemingly decentralized. Neither trusting users, nor authoriy only is applicable. A domain name may be resolved to different addresses in different parts of the network. This is actually inevitable, since if you allow multiple name resoltuion service, there is always inconsistency. In conclusion, WoT is natural and singular consensus is unnecessary and impractical. In practice, we use sites as the main entities in name resolution system, called name provider. A name provider site can be a group of users, a blockchain, a static mapping table, or even only a script. Name provider site (from centralized to decentralized) Centralized, and personally issued, or by organization. Blockchain, a variation of centralized name provider Group of selected users. Static resolution table First-come-first-serve script based on WoT. Subjective WoT (not a site) The form of user group name provider is a kind of delegated trust. The user can either trust all of the users specified in the site, or none of them, which unifies name resolution more than WoT only. The group doesn’t check and confirm every name, but only bans invalid names, due to limited efforts. A corrupted name provider is soon replaced with a new one, which can’t happen in the centralized web due to the ranking algorithm. First-come-first-serve pattern is only possible after sybil attack problem is eliminated, which requires WoT. It’s one of the examples of using only script as a name provider, which processes WoT name results and accepts the very first records. Distributed searchDistributed search is performed after a local search. Due to the efficiency of possible DoS on it, this may happen only within WoT. To search on a site, or all sites, the requester sends a search request on channel, and collects and sorts the search results sent by other peers. The ranking algorithm can be based on evalutated trust value and seeder count, which is a natural metric in dweb, or combined with other methods like PageRank. No blockchain economics strategy is applied for obvious reasons explained below. In case anyone thinks some item is good or bad, he could mark it to increase or decrease the trust value of the peers responsible for that item. When the user starts a search, the request is sent over channel protocol. Any peer wiiling to on condition that there aren’t enough results for this search would return a list of result individually, ranked according to seeder count etc. For the next page, the user has to send another request. The final list is aggregated from collected lists and sorted by trust value. The amount of trust is decreased or increased is relevant to the position of the item or how much the peer advocates it. The evaluation of WoT should be ‘separated’ for different purposes. For example, some users prefer detailed content as beginners for some topic, but as experts in other topics, with different preference. We shouldn’t distrust that peer for this reason. So the peer should pass some preference arguments when searching, which is inferred during prior user responses. The responding peers are given more chances if the user preference is undetermined. For later requests, the results should match the preference since we’ve provided such a hidden parameter, or otherwise the peers can be safely distrusted then. The same applies to all kinds of content sorting. Peers are responsible for tagging content, such as NSFW, and other tags some people dislike. This produces a situation that some content are served but not displayed, which is intended and limited on size. A search request contains the signature and preference data of the requester, paging data, and TTL. In case anyone wants to the search the entire network, he can start a long-term search, which includes the fields to prevent repeated responses. In other words, search may act as a crawler. Structured algorithmMany decentralized search engines use a broadcasting search strategy, which leads to exponential growth of requests. For any query, there should be a way to locate a subset of peers who are more possible to have the desired result. Most requests can be optimized with those decentralized indices. Target of a search: Local sites, objects Non-downloaded sites Non-downloaded objects from a site Related objects for a known object Distributed indices For non-downloaded sites, we need network-wide indices to locate the desired results. When a hit is found in a site either locally or by delegated peers, since the data within a site is usually related, the rest of the site is searched too, for there might be better results than the found one. An object has all its links used in its content listed, so there won’t be an extra step of extracting links from the content. TODO: Decentralized indexing algorithm FeedIf search is considered initiative, then feed is passive. As how WoT does, feed also happens in WoT, and is a core component of melotte. A site uses channel protocol to release updates, but the underlying network is not guaranteed to be as large as melotte network. Furthermore, a user who hasn’t joined some site can’t receive its updates. Feed can be seen as a site in some way. However, for this type of site, it should be specialized and integrated into the core. We don’t want to see many competing sites for this feature, which splits the network, causing many unnecessary problems. Feed is fundamentally different from site channels, as notifications from a site are processed and filtered by the site. Feed works on the basis of users, which is exactly the same way search does. The content of feed depends on the WoT, the users you trust, not the site. It won’t be boring release notes from sites, but distinct unique feeds published and composed by your WoT. In order to prevent feed being abused like social networks, the tolerance of spam is default to be very low. There won’t be feed comments, feed upvotes, which makes the network less reliable and performat, allowing feed to work even by polling on DHT with existing knowledge of WoT peers. Feed is a service from Melotte. It can have different frontends, and be used by sites to have additional features. The backend of feed always remain the same, as it serves only as a basic protocol. Both feed and search can be extended by a site, to have comments, ratings, but the basic protocol supports only non-aggregated data from beginning to end. PrivacyWoT naturally advocates using one single identity, or otherwise you could only get some weak trust. It is ok to be anonymous but not feasible to be non-traceable, since WoT is trustful, unlike the trustless blockchain. A user has to be traceable to get trusted. Activities like search require sending preference information, which increases the probability of getting desired result. You may also disable this, but the search would be less useful. Permission Core, where everything is allowed and privatekeys are stored. Plugins, such as an additional obfuscation protocol. Sites, sandboxed in WASM. A site may change values in WoT, which is located in core. Typically, operations on WoT happen every time the user upvotes something or does something similiar. We notify and prompt the user conventionally, which often annoys the user. On the other side, allowing a site to do that after only one confirmation neglects the potential risk of a site pretending to be honest only at the first time. Possible attacks: Set someome’s trust value to zero, so he is globally blocked. Remove all other records and trust the owner of a site. Add thousands of trust records, all of which trust the owner of a site. Firstly, we need a log for all operations. In other words, each site has its own virtual WoT table. The second attack doesn’t happen then. For any site who attempts to flood the WoT, a warning is shown to the user once the operations exceed a limit. Prompt user the first time a site uses some priviledge Generate a report on what the site has done after a random time period Show warning when any possible abuse is detected Types of priviledges: Channel and block protocol, basic needs Operations on other sites, eg. add, remove, call, read, write Access to core functionalities, eg. WoT, search Install/Remove plugins System IO, etc. Internet access An object or a site may have a WWW link specified. If the user has configured to allow the use of centralized network, the link is used as a block protocol. A site may also request a block with a WWW link, especially for a decentralized package manager. The link isn’t necessarily fixed into the block, as there are many npm registries serving the same copies. For a same type of objects, they might share the same scheme of composing links. So, an object may have a link that is a CID which refers to another object who has the code to compose a link. Mirroring WWWMelotte doesn’t naturally support indexing WWW pages, unlike YaCy, but users are free to use a utility provided by Melotte to copy the pages onto the network. When the user notices something great on WWW, he could bookmark it in Melotte browser, and the pages are automatically indexed. An mirrorred object from WWW may have a field pointing to the soruce urls, for further synchronization. Time guaranteeThis section describes a way to achieve an As Sound As Possible timestamp (ASAP) via an As Soon As Possible false timestamp rejection, for site content. A site is firstly signed on its content, blocks. When the signer decides to publish the site, he signs a metadata that links to the blocks of the site, and propagate the metadata via channel protocols. Typically, channel protocols need peers to forward the metadata. We call the peers that get the metadata from the site signer directly, the first layer, and the peers that get the metadata from these peers the second layer. In a channel protocol that doesn’t require metadata forwarding, there are always offline peers which require forwading. Note that the metadata being forwarded is always that metadata signed by the original signer. Denote the time the author signs the blocks with T(block), the timestamp he writes in the metadata as T(meta), and the time the first layer receieves the metadata as T(1), and the second layer as T(2). Denote the time you receives the metadata as T(you) and the now as Now When the metadata is published, if the timestamp is fake, by protocol, all well-behaved peers will reject and drop the false metadata. In case any peer in the first layer wrongly propagates the metadata into the second layer, the first layer fails and Now has increased. The peers can’t trust other peers, so they can only trust their own clocks. Assume there are N malicious nodes evenly distributed and the connected peers are randomly selected, so the probability to connect to a malicious node is N/totalNodes. Denote it as P. Metadata without a timestamp field will be instantly dropped, probability 1-P. The first layer fails to reject false metadata, P, and the second layer P², and the layer n Pⁿ. On layer n , the probability that layer drops false metadata is 1 - Pⁿ. For instance, P is 0.1, on the layer two the probability of rejecting false timestamps is already 99%, so that all timestamps pass through layers satisfy condition T(meta) &lt; T(2) , at least. It is T(2), but not T(1). See paragraphs below. Why a timestamp in metadata is necessary ? As we know, each layer verifies the timestamp before propagating to another layer, which prevents false information from spreading. If there is timestamp built in metadata, validating timestamp takes less then a second, and we can get the result, to propagate or not. Block protocols are always slow, however. Let the time to look up timestamp inside the block be Tx. In the first layer, the condition to check timestamp inevitably becomes T(meta) &lt; Tx + T(1), rather than T(meta) &lt; T(1). Intuitively, Tx can range from 30 seconds to five minutes. Tx has been added to T(1) in the condition, indicating we gave the metadata more tolerance, because the metadata can have a false timestamp T(1) &lt; T(meta) &lt; Tx + T(1) which is valid in this situation. You may say we have intended tolerance for the local time can’t be accurate, but this tolerance accumulates. In the layer 2, for T(2) is approximately Tx + T(1), it is T(meta) &lt; 2Tx + T(1). Besides this cost, every peer that propagates the metadata has to query DHT and download the block, which is very expensive and vulnerable to DoS attack. Through this process, the timestamp in the metadata can be thought as valid. When you get the timestamp and download the blocks, you check the blocks against condition T(block) &lt; T(meta), because T(meta) &lt; T(1) is always true. Time gurantee only checks the publish time, the time the content is exposed to the public. The purpose of checking T(meta) is to enforce the author to include a timestamp which statisfies T &lt; T(1)(approximately). A timestamp is allowed to become valid although It was invalid. Suppose we have a timestamp Now + 5 days, it is invalid within following 5 days and is immediately dropped and not propagated in case anyone tries to publish it. After 5 days, the metadata can be published, so the publish time can be considered correct, though it was generated five days ago. The validation before propagation prevents any wrong publish time. In many cases, the scenario that the publish time of something is unclear, and something inside it suddenly becomes valid after a certain period, is undesired. Will you get false timestamp ? It depends on your ‘location’ in the network, and whether you are the first time visiting a site. For backward false timestamp, any such attempt is prohibited by corresponding site validation script, as long as you have the original content that contains the ‘true’ timestamp (the first timestamp is considered true, see below). It’s impossible to do time guarantee on site content, because this requires to download the new content. Regarding ‘future’ false timestamp, it is already suppressed by T(block) &lt; T(meta) &lt; T(1). If you are next to the false timestamp sender, you can always detect and drop that metadata. It is still possible to have a false timestamp that T(1) &lt; T(meta) &lt; T(you), when the delay on layers are significant, since Now constantly increases. Let’s call this delay timestamp. The more layers, the more delay, hence more unwanted tolerance. Except for delay timestamp, the probability to recognize false future timestamp as valid is 0. Can we prevent backward timestamps ? Yes, we can. We have multiple options, the one is to validate after propagation and downloading a site, the other one is to validate during propagation. The former has the risk of history being archived. In contrast, the latter prevents false timestamps in the first place, which is arguably better as it doesn’t even acknowledge the existence of false timestamps. A site that cares about the correctness of timestamps can use metadataScript to validate timestamps wtih one more condition against backward false timestamps before propagating metadata (on object level not block). This is the solution to backward timestamp. For backward timestamp, only peers with corresponding site downloaded and know the original timestamp can validate, unlike future timestamp validation where every peer knows the condition to validate, which is T(meta) &lt; Now. Fortunately, the site propagation protocol has a channel for each site respectively. Only the content of that site can be published on that channel, so false timestamps won’t be mistakenly propagated in any other channel which doesn’t accept those timestamps at all. Notice that the timestamp in the metadata is still necessary, because the block referenced in metadata is always undownloaded at the moment you receive that metadata; but in backward timestamp validation, you already have the site content downloaded since you have subscribed to that channel. Also, there may be peers who don’t have site downloaded, as new comers. The solution is simple, in that time-sensitive site, disallow the new comers to propagate metadata, although this may reduce connectivity. The mechanism to validate backward timestamp is basically the same to future timestamp. Denote the target timestamp is T(back), and the ‘correct’ timestamp is T(prev). We use condition T(prev) == T(back), as published timestamps can’t be changed. The probability of each layer of successful rejection is the same as future timestamp. What if I am the new comer of a site ? This is the only possible scenario where you might get a false backward timestamp. You already have the awareness of Now in the condition of future timestamp validation; however, you don’t have the knowledge of the original timestamps in a site. As a result, you won’t get a false future timestamp. If the blocks are complete, ie. not archived or pruned, the data script can automatically detect any attempt to modify existing timestamps. The probability of getting false timestamp as a new comer is P, assuming all malicious nodes are united to give you false timestamps of that site. Possible solutions include requesting site metadata from multiple peers, and compare to check if they are identical. Download the history if the condition fails. As the N malicious nodes are distributed evenly, the probability reduces if we request from more nodes. For k times of metadata requests or answers, the probability of getting false timestamp is P^k. In fact, we already request from multiple peers, because there are always peers answering requests at the same time. Sybil attack ? There can’t be sybil attack, since creating massive identities don’t help. This is not a reputation system. If there are enough layers, false timestamps eventually vanish. If not, the better, everyone can validate the timestamps on their own, as they know Now and original timestamps. In conclusion, both backward and future timestamps are banned, in time sensitive sites which limit the metadata to be propagated only among peers who have downloaded that site. Cases Some examples Simple announcement board Anyone can publish content with size limit less than 256 bytes Not using mutable storage. The only block in mgmt chain is genesis block, that contains a data script allowing only a certain CID of backend and frontend, and user content is size limited. Backend and frontend are unixfs format, not delta-encoded. The mgmt script rejects all later updates. WoT is enabled by default. This kind of site is simple enough that can be immutable. Maybe we don’t need that publickey, as the site address. Simple chat Messages are immuatble. Each user has a editable bio. Code base is mutable for future versions Blockchain with smart contract Mutable storage for wallet UI and broadcasted transactions, and immutable storage for the blockchain itself. Smart contracts are run by calling melotte API. Other sites can call this blockchain via melotte. Chat Rotating mutable message storage. Two step message publishing, on channel firstly for lower delay, and then message object for long term storage. Each peer running this site has a overall size limit for all message objects served. When new message objects arrive, old objects are pruned, and no longer get distributed. WoT result is applied and different users have different limit. Since message object is for relaying when the party is not online, message objects temporarily stored in other peers are soon marked as prunable after the party is confirmed to be online. It is consider to be online when the party who was offline relplies. Forum Object types: topic, comment, reaction, attachment, user, signal (for reporting and moderating) A topic has topic name and body. A comment has a link to the topic. Orphaned topics are garbage-collected. A reaction is like a comment, but for upvote and downvote. An attachment is a wrapper for files that referenced in the topic or comment, which contains metadata of the target. Moderators can use signals to manage the site. Signals are also used to report spam, and inappropriate content. All things are editable. Only the owners specified in management chain have the rights to archive. Moderators are not registered in the management chain, but in the site data part. Wiki Object types: page, index, user, comment, signal A user has a username associated and some basic information stored in user object. A wiki page is an object that has many versions, publised by any user. A comment links to the related wiki page. Otherwise it will be garbage-collected. Maybe there should be a metadata page object that has the links to multiple pages in various languages. WoT is applied. Wiki page editions with low WoT score is hidden, until it gets enough approval, via signal, from trustworthy peers in WoT (subjective). The wiki should embed a forum, and that forum should embed a chat. Weird, right ? Live collaborative editing, google-docs like Making use of NAT traversal ability of the network. Integrate with IM. Git Git has to be simulated on the top of the git-like object-version structure, because git treat the whole repository as an ‘object’ while we track objects individually, which is more suitable and efficient in dweb. Root, metadata git repo object that links to other objects Repo name, description, readme and owners, size-limited. Only metadata objects are downloaded by all peers. When someone vistis a repo, the repo is seeded. Shallow seed can be applied automatically if a peer has limited storage, ie. archiving old commits. Local search based on repo name, description and readme. A lightweight full-text indexer is needed. Extended markdown without compromising security. Stars are evaluated with the WoT score of users. Trending list is automatically generated with the data from local tracker that tracks the taste of the user. It is possible to get the data of ‘rising’, since timestamp is guaranteed. Private repos are totally ok. Melotte can even be a private network. Issues page is similar to a forum, but it can have better a integration with project kanban board. Social networks Object types: user, post, comment, reaction, group Sorting by time, popularity, preference, random, or any of these combined. Posts from a group are only downloaded when requesting How shall we rank and sort the content ? For different purposes, the method should differ. Websites like reddit, quora rank the content by populariry, which is not applicable in the case of stackoverflow. The point is popularity doesn’t imply quality. Among the users, the standard of quality content varies. Common factors of ranking are time, popularity, quality and preference. For instance, social networks can adjust the algorithm to favor popularity. Quality is actually a vague word. It may stand for populariry sometimes. But popular opinion is not always true. A repository with more stars might be worse instead, because the people who rate the repository aren’t necessarily familiar with the technology the repository is about, and popular repositories tend to get more stars. That’s the flaws of current ranking systems. As a result, it takes a long time for a new project to be known to the others. To make things worse, some monopolistic search engines in some countries rank the sites by bidding. Now the definitions of the two terms are clear, quality is the preference of the WoT community/group you are in, plus some algorithm that counts citation/dependency/PageRank, and preference is about yourself. The spam articles I dislike might be the taste of some readers, on the other hand. Philosophy of this projectMany p2p projects have been started in this decade, for different purposes and applications. No one has ever tried or wanted to build an infrastruture to replace centralized web. As the demand changes, people invent new protocols, and old networks are abandoned. A remarkable attempt is libp2p and IPFS. They aim to modularize the foundations of p2p protocols, eg. DHT. It’s far from done, however. That’s only a part of the dweb protocol stack. Following the trend of putting things into the web, we adopted WASM. Therefore, a site is self-hosted, sandboxed, and acts like a native application. Melotte is not a bittorrent video streamer, or a decentralized file synchronizer, or a distributed search engine. It’s all of them. In regard to compatibility, we add one more layer, inluding block protocol and channel protocol. Thus, both bitswap and bittorrent can be block protocols. That’s flexibility, one of the features we aim to offer. Besides the concept of immuatble storage from IPFS, we also introduced object, site, and many more. Blockchain can never replace the decentralized web, no matter how overwhelming the hype is. The nature of blockchain that it is validated and dominated by the minority, the rich, whether it’s PoW, PoS or proof of anything, deciding that it is impossile to be censorship-resistant, which is contrary to the original purpose of p2p networks, while existing projects use blockchains nearly everywhere. Even worse, many blockchains still use wasteful and non-sense Proof of Work protocol, which could only centralize it more. Melotte is ‘No-Database’, one step beyond Nosql. In contrast to ZeroNet/Orbit-db/GUN, we don’t invent an abstraction of database. Database, as a concept, only belongs to the centralized web. What is a good design? No unnecessary abstraction Layered, so the flexibility rises for lower layer Different solutions covering different situations For 3, we have many protocols to use, even in the worst network condition, so that the network isn’t shut down when the connectivity drops but keeps working with tradeoffs, such as speed. Gossip WoT routing DHT Tracker Centralized block protocol One kind of layering is unnecessary, for example, orbit-db, which makes IPFS more like a database, but produces no extra functionalities. The 2 is what most projects don’t follow. BitTorrent aimed for file sharing, but it is acutally a data exchanging protocol. This is not the fault of wording, because a data exchanging protocol is more general. The purpose of file sharing limits the use of BitTorrent, determining how the software is designed. SSB directly builds applications without middle layers like what IPFS has, the clear distinction between layers. The consequence of mixing layers is obvious that it makes partly resuing the protocol much harder. Few project has done the 3, probably because most projects were started in countries without such worry and need; however, the combination of protocols tends to maximize the stability and performance for each condition. There’s no excuse not to use DHT when the connection works. Conceptual centralizationBlockchain has a centralization on influence and power, projects have centralization on development and sites are centralized to the owners. Nothing can be really decentralized while retaining reasonable features. On the other hand, the degree of centralization depends on your need. BitTorrent can be completely decentralized with the benefits of DHT, which is a exact mapping between hashes and files. There’s no disagreement in this and everything is automated. A site can’t be the same, for it needs maintainence, and the users sometimes disagree on the management. The private-ownership based model is introduced by blockchain, that the majority follow the minority with more money. Other P2P networks found a solution of using WoT. WoT doesn’t address the disagreement, but keeps it. This particularly useful for tasks like filtering spam and getting content based on personal preference, which is dealt with blockchains before by other projects. In the end, we finally need something with strong consistency, such as names, which would cause much inconvenience without an agreement. It doesn’t mean we must use a blockchain. Rather, on the basis of a site, we may have a group of users representing the will of the users of that name provider. The users trust the group voluntarily, so that it doesn’t have real authority. Whenever the group violates the will of the users, the competitors win, with the help of the user-centered ranking algorithm. The site owners know this logic, so that they won’t do this in the first place. Censorship HoW do we cope with censorship There’re two types of connection: Domestic connection International connection Three types of filter, and usually some of them are combined in practice: Blacklist, which is often used in developing countries as a basic censorship method. Dynamic, eg. keyword based filter, AI. Whitelist, deployed only in some very authoritarian countries. Methods of censorship: Manual censoring Deep packet inspection Statistical analysis Active probing and the consequence: Port ban IP ban DNS spoofing QoS, causing packet loss Banning international connection greatly affects commerce, which is not an affordable option for most nowaday countries, but the governments do want to censor such connections. Therefore they set up blacklist and dynamic filter. It’s unlikely to use a whitelist. As long as they don’t use a whitelist, we will have an obfuscation protocol sooner or later, which cheats the filter, pretending to be some normal connection. It is much harder to block domestic connection, as it requires more censorship devices, and deployed in every city. Except for the problem of the lack of public IP, they can’t detect what servers are running Melotte and what connections are P2P, especially in gossip mode. Public IP issue is automatically solved when we have international connection.For less developed countries, the people can even use sneakernet and mesh network. An obfucation protocol can probably cheat packet inspection, and some higer level obfucastion may pass statistical analysis, which is already better than centralized proxies for we have more IPs. When they’ve noticed the threat, they may do manual censoring, and write some active probing mechanism targeted us. For example, running a node in DHT, and collecting all IPs for banning. The same thing doesn’t happen to a gossip network, since a node can have only a small amount of IPs. The IPs are also distributed more equally throughout the IP universe, than a centralized proxy provider. Banning an IP range would cause much damage. In conclusion, both international connection and domestic connection can’t be easily censored with an up to date obfuscation protocol. Besides the resistance from the transport, the management of a site can be voting. We may create a site with owners from different countries. Even if the privatekey of one owner is compromised, the site isn’t taken down easily. Even if the site is really taken down, a new site can be started based on the original site, the historical version, with a new management group. The users will check and choose the new site. AnonymityProtocols to support: Tor I2P(Kovri) In the future, a new protocol will be developed based on the WoT. MiscellaneousIn some way, we borrowed the idea of site from ZeroNet, content-addressing and modulariry from IPFS, and WoT from SSB. Publickey is a multiformat. PeerId is not used in most cases, because it is a format designed for IPFS, and it can always be calculated from Publickey.","categories":[],"tags":[{"name":"dweb","slug":"dweb","permalink":"https://planetoryd.github.io/tags/dweb/"},{"name":"archived","slug":"archived","permalink":"https://planetoryd.github.io/tags/archived/"}]}],"categories":[],"tags":[{"name":"advertising","slug":"advertising","permalink":"https://planetoryd.github.io/tags/advertising/"},{"name":"final","slug":"final","permalink":"https://planetoryd.github.io/tags/final/"},{"name":"dweb","slug":"dweb","permalink":"https://planetoryd.github.io/tags/dweb/"},{"name":"draft","slug":"draft","permalink":"https://planetoryd.github.io/tags/draft/"},{"name":"censorship","slug":"censorship","permalink":"https://planetoryd.github.io/tags/censorship/"},{"name":"diagram","slug":"diagram","permalink":"https://planetoryd.github.io/tags/diagram/"},{"name":"trust","slug":"trust","permalink":"https://planetoryd.github.io/tags/trust/"},{"name":"blockchain","slug":"blockchain","permalink":"https://planetoryd.github.io/tags/blockchain/"},{"name":"china","slug":"china","permalink":"https://planetoryd.github.io/tags/china/"},{"name":"archived","slug":"archived","permalink":"https://planetoryd.github.io/tags/archived/"}]}